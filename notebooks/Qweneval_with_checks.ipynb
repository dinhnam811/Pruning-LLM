{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Following Evaluation with Check Functions\n",
    "\n",
    "Evaluates model responses using programmatic constraint checkers instead of LLM judge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 samples\n",
      "\n",
      "First sample:\n",
      "Prompt: Write a 300+ word summary of the wikipedia page \"https://en.wikipedia.org/wiki/Raymond_III,_Count_of...\n",
      "Constraints: ['punctuation:no_comma', 'detectable_format:number_highlighted_sections', 'length_constraints:number_words']\n",
      "Kwargs: {'num_highlights': None, 'relation': None, 'num_words': None, 'num_placeholders': None, 'prompt_to_repeat': None, 'num_bullets': None, 'section_spliter': None, 'num_sections': None, 'capital_relation': None, 'capital_frequency': None, 'keywords': None, 'num_paragraphs': None, 'language': None, 'let_relation': None, 'letter': None, 'let_frequency': None, 'end_phrase': None, 'forbidden_words': None, 'keyword': None, 'frequency': None, 'num_sentences': None, 'postscript_marker': None, 'first_word': None, 'nth_paragraph': None}\n"
     ]
    }
   ],
   "source": [
    "# Load from Hugging Face dataset\n",
    "ds = load_dataset(\"wis-k/instruction-following-eval\")\n",
    "train_ds = ds[\"train\"]\n",
    "data = train_ds.select(range(30))  # Select 30 samples\n",
    "\n",
    "print(f\"Loaded {len(data)} samples\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"Prompt: {data[0]['prompt'][:100]}...\")\n",
    "print(f\"Constraints: {data[0]['instruction_id_list']}\")\n",
    "print(f\"Kwargs: {data[0]['kwargs'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Check Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 20 check functions\n"
     ]
    }
   ],
   "source": [
    "def check_punctuation_no_comma(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if response contains no commas.\"\"\"\n",
    "    has_comma = ',' in response\n",
    "    count = response.count(',')\n",
    "    return not has_comma, f\"Commas: {count}\"\n",
    "\n",
    "def check_highlighted_sections(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of highlighted sections using *text* format.\"\"\"\n",
    "    num_required = kwargs.get('num_highlights', 0)\n",
    "    highlights = re.findall(r'\\*[^*]+\\*', response)\n",
    "    count = len(highlights)\n",
    "    return count >= num_required, f\"Highlights: {count}/{num_required}\"\n",
    "\n",
    "def check_number_words(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check word count constraint.\"\"\"\n",
    "    words = len(response.split())\n",
    "    required = kwargs.get('num_words', 0)\n",
    "    relation = kwargs.get('relation', 'at least')\n",
    "\n",
    "    if relation == 'at least':\n",
    "        passed = words >= required\n",
    "    elif relation == 'less than':\n",
    "        passed = words < required\n",
    "    else:\n",
    "        passed = words == required\n",
    "\n",
    "    return passed, f\"Words: {words} ({relation} {required})\"\n",
    "\n",
    "def check_number_placeholders(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of placeholders like [address], [name].\"\"\"\n",
    "    num_required = kwargs.get('num_placeholders', 0)\n",
    "    placeholders = re.findall(r'\\[[^\\]]+\\]', response)\n",
    "    count = len(placeholders)\n",
    "    return count >= num_required, f\"Placeholders: {count}/{num_required}\"\n",
    "\n",
    "def check_repeat_prompt(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if prompt is repeated at the beginning.\"\"\"\n",
    "    prompt_to_repeat = kwargs.get('prompt_to_repeat', '')\n",
    "    if not prompt_to_repeat:\n",
    "        return True, \"No prompt to repeat\"\n",
    "    response_start = response.strip()[:len(prompt_to_repeat)]\n",
    "    passed = response_start == prompt_to_repeat.strip()\n",
    "    return passed, f\"Prompt repeated: {passed}\"\n",
    "\n",
    "def check_title_format(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if response contains title in <<title>> format.\"\"\"\n",
    "    has_title = bool(re.search(r'<<[^>]+>>', response))\n",
    "    return has_title, f\"Title found: {has_title}\"\n",
    "\n",
    "def check_english_lowercase(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if entire response is lowercase.\"\"\"\n",
    "    has_uppercase = any(c.isupper() for c in response)\n",
    "    return not has_uppercase, f\"All lowercase: {not has_uppercase}\"\n",
    "\n",
    "def check_number_bullet_lists(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of bullet points using * format.\"\"\"\n",
    "    num_required = kwargs.get('num_bullets', 0)\n",
    "    bullets = re.findall(r'^\\* ', response, re.MULTILINE)\n",
    "    count = len(bullets)\n",
    "    return count == num_required, f\"Bullets: {count}/{num_required}\"\n",
    "\n",
    "def check_english_capital(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if entire response is uppercase.\"\"\"\n",
    "    letters = [c for c in response if c.isalpha()]\n",
    "    if not letters:\n",
    "        return False, \"No letters\"\n",
    "    all_upper = all(c.isupper() for c in letters)\n",
    "    return all_upper, f\"All uppercase: {all_upper}\"\n",
    "\n",
    "def check_multiple_sections(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of sections with splitter.\"\"\"\n",
    "    num_required = kwargs.get('num_sections', 0)\n",
    "    splitter = kwargs.get('section_spliter', 'SECTION')\n",
    "    sections = re.findall(rf'{splitter}\\s*\\d+', response, re.IGNORECASE)\n",
    "    count = len(sections)\n",
    "    return count >= num_required, f\"Sections: {count}/{num_required}\"\n",
    "\n",
    "def check_capital_word_frequency(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check frequency of all-caps words.\"\"\"\n",
    "    capital_frequency = kwargs.get('capital_frequency', 0)\n",
    "    capital_relation = kwargs.get('capital_relation', 'at least')\n",
    "\n",
    "    words = response.split()\n",
    "    capital_words = [w for w in words if len(w) >= 2 and w.isupper() and w.isalpha()]\n",
    "    count = len(capital_words)\n",
    "\n",
    "    if capital_relation == 'at least':\n",
    "        passed = count >= capital_frequency\n",
    "    elif capital_relation == 'less than':\n",
    "        passed = count < capital_frequency\n",
    "    else:\n",
    "        passed = count == capital_frequency\n",
    "\n",
    "    return passed, f\"Capital words: {count} ({capital_relation} {capital_frequency})\"\n",
    "\n",
    "def check_quotation(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if wrapped in quotes.\"\"\"\n",
    "    response_stripped = response.strip()\n",
    "    has_quotes = response_stripped.startswith('\"') and response_stripped.endswith('\"')\n",
    "    return has_quotes, f\"Quoted: {has_quotes}\"\n",
    "\n",
    "def check_keywords_existence(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if keywords exist.\"\"\"\n",
    "    keywords = kwargs.get('keywords', [])\n",
    "    if not keywords:\n",
    "        return True, \"No keywords\"\n",
    "    response_lower = response.lower()\n",
    "    missing = [k for k in keywords if k.lower() not in response_lower]\n",
    "    return len(missing) == 0, f\"Missing: {missing}\" if missing else \"All present\"\n",
    "\n",
    "def check_json_format(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if valid JSON.\"\"\"\n",
    "    try:\n",
    "        response_stripped = response.strip()\n",
    "        if response_stripped.startswith('```'):\n",
    "            response_stripped = re.sub(r'^```(?:json)?\\s*', '', response_stripped)\n",
    "            response_stripped = re.sub(r'```\\s*$', '', response_stripped)\n",
    "        json.loads(response_stripped)\n",
    "        return True, \"Valid JSON\"\n",
    "    except:\n",
    "        return False, \"Invalid JSON\"\n",
    "\n",
    "def check_number_paragraphs(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of paragraphs.\"\"\"\n",
    "    num_required = kwargs.get('num_paragraphs', 0)\n",
    "    relation = kwargs.get('relation', 'at least')\n",
    "    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n|\\*\\*\\*', response) if p.strip()]\n",
    "    count = len(paragraphs)\n",
    "\n",
    "    if relation == 'at least':\n",
    "        passed = count >= num_required\n",
    "    else:\n",
    "        passed = count == num_required\n",
    "\n",
    "    return passed, f\"Paragraphs: {count} ({relation} {num_required})\"\n",
    "\n",
    "def check_two_responses(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check for two parts separated by ******.\"\"\"\n",
    "    parts = response.split('******')\n",
    "    return len(parts) >= 2, f\"Parts: {len(parts)}/2\"\n",
    "\n",
    "def check_response_language(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Language check (simplified).\"\"\"\n",
    "    lang = kwargs.get('language', 'en')\n",
    "    return len(response.strip()) > 0, f\"Lang check ({lang})\"\n",
    "\n",
    "def check_letter_frequency(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check letter frequency.\"\"\"\n",
    "    letter = kwargs.get('letter', '')\n",
    "    frequency = kwargs.get('let_frequency', 0)\n",
    "    relation = kwargs.get('let_relation', 'at least')\n",
    "\n",
    "    if not letter:\n",
    "        return True, \"No letter\"\n",
    "\n",
    "    count = response.count(letter)\n",
    "\n",
    "    if relation == 'at least':\n",
    "        passed = count >= frequency\n",
    "    elif relation == 'less than':\n",
    "        passed = count < frequency\n",
    "    else:\n",
    "        passed = count == frequency\n",
    "\n",
    "    return passed, f\"'{letter}': {count} ({relation} {frequency})\"\n",
    "\n",
    "def check_end_checker(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check ending phrase.\"\"\"\n",
    "    end_phrase = kwargs.get('end_phrase', '')\n",
    "    if not end_phrase:\n",
    "        return True, \"No end phrase\"\n",
    "    passed = response.strip().endswith(end_phrase)\n",
    "    return passed, f\"Ends correctly: {passed}\"\n",
    "\n",
    "def check_forbidden_words(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check forbidden words.\"\"\"\n",
    "    forbidden = kwargs.get('forbidden_words', [])\n",
    "    if not forbidden:\n",
    "        return True, \"No forbidden words\"\n",
    "    response_lower = response.lower()\n",
    "    found = [w for w in forbidden if w.lower() in response_lower]\n",
    "    return len(found) == 0, f\"Forbidden: {found}\" if found else \"None\"\n",
    "\n",
    "# Mapping\n",
    "CONSTRAINT_CHECKERS = {\n",
    "    'punctuation:no_comma': check_punctuation_no_comma,\n",
    "    'detectable_format:number_highlighted_sections': check_highlighted_sections,\n",
    "    'length_constraints:number_words': check_number_words,\n",
    "    'detectable_content:number_placeholders': check_number_placeholders,\n",
    "    'combination:repeat_prompt': check_repeat_prompt,\n",
    "    'detectable_format:title': check_title_format,\n",
    "    'change_case:english_lowercase': check_english_lowercase,\n",
    "    'detectable_format:number_bullet_lists': check_number_bullet_lists,\n",
    "    'change_case:english_capital': check_english_capital,\n",
    "    'detectable_format:multiple_sections': check_multiple_sections,\n",
    "    'change_case:capital_word_frequency': check_capital_word_frequency,\n",
    "    'startend:quotation': check_quotation,\n",
    "    'keywords:existence': check_keywords_existence,\n",
    "    'detectable_format:json_format': check_json_format,\n",
    "    'length_constraints:number_paragraphs': check_number_paragraphs,\n",
    "    'combination:two_responses': check_two_responses,\n",
    "    'language:response_language': check_response_language,\n",
    "    'keywords:letter_frequency': check_letter_frequency,\n",
    "    'startend:end_checker': check_end_checker,\n",
    "    'keywords:forbidden_words': check_forbidden_words,\n",
    "}\n",
    "\n",
    "print(f\"✓ Loaded {len(CONSTRAINT_CHECKERS)} check functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation function ready\n"
     ]
    }
   ],
   "source": [
    "def evaluate_single(response: str, instruction_id_list: List[str], kwargs_list: List[Dict]) -> Dict:\n",
    "    \"\"\"Evaluate response with check functions.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, constraint_type in enumerate(instruction_id_list):\n",
    "        kwargs = kwargs_list[i] if i < len(kwargs_list) else {}\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        \n",
    "        check_func = CONSTRAINT_CHECKERS.get(constraint_type)\n",
    "        \n",
    "        if check_func:\n",
    "            try:\n",
    "                passed, explanation = check_func(response, kwargs)\n",
    "            except Exception as e:\n",
    "                passed = False\n",
    "                explanation = f\"Error: {str(e)}\"\n",
    "        else:\n",
    "            passed = False\n",
    "            explanation = f\"Unknown: {constraint_type}\"\n",
    "        \n",
    "        results.append({\n",
    "            'constraint': constraint_type,\n",
    "            'passed': passed,\n",
    "            'explanation': explanation\n",
    "        })\n",
    "    \n",
    "    total = len(results)\n",
    "    passed_count = sum(r['passed'] for r in results)\n",
    "    pass_rate = passed_count / total if total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'passed': passed_count,\n",
    "        'total': total,\n",
    "        'pass_rate': pass_rate\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 3E16-8FE4\n",
      "\n",
      " Directory of C:\\Users\\namnd\\.cache\\huggingface\\hub\n",
      "\n",
      "09/12/2025  14:20    <DIR>          .\n",
      "09/12/2025  14:20    <DIR>          ..\n",
      "04/12/2025  13:57    <DIR>          .locks\n",
      "09/12/2025  14:20    <DIR>          datasets--wis-k--instruction-following-eval\n",
      "09/12/2025  14:28    <DIR>          models--Qwen--Qwen2.5-Coder-3B-Instruct\n",
      "               0 File(s)              0 bytes\n",
      "               5 Dir(s)  347,849,576,448 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir \"C:/Users/namnd/.cache/huggingface/hub/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:/Users/namnd/Documents/QwenCoder-40...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\u001b[32m      7\u001b[39m quantization_config = BitsAndBytesConfig(\n\u001b[32m      8\u001b[39m     load_in_8bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m     bnb_8bit_compute_dtype=torch.float16\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda:0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m model.eval()\n\u001b[32m     18\u001b[39m torch.cuda.empty_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\modeling_utils.py:4881\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4872\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename.endswith(\n\u001b[32m   4873\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4874\u001b[39m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors.index.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   4875\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4876\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4877\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4878\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_explicit_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   4879\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4881\u001b[39m hf_quantizer, config, dtype, device_map = \u001b[43mget_hf_quantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4885\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4886\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4887\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4888\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\quantizers\\auto.py:319\u001b[39m, in \u001b[36mget_hf_quantizer\u001b[39m\u001b[34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[39m\n\u001b[32m    316\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     dtype = hf_quantizer.update_dtype(dtype)\n\u001b[32m    327\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_8bit.py:73\u001b[39m, in \u001b[36mBnb8BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m     )\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available(check_library_only=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     )\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     78\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe bitsandbytes library requires PyTorch but it was not found in your environment. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can install it with `pip install torch`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     80\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "MODEL_PATH = \"C:/Users/namnd/Documents/QwenCoder-40\"\n",
    "\n",
    "print(f\"Loading {MODEL_PATH}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda:0\",\n",
    "    low_cpu_mem_usage=True \n",
    ")\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"✓ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generation function ready\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt: str, max_tokens: int = 800, temperature: float = 0.4) -> str:\n",
    "    \"\"\"Generate response using chat template.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"assistant\" in full_text:\n",
    "        response = full_text.split(\"assistant\")[-1].strip()\n",
    "    else:\n",
    "        gen_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        response = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"✓ Generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda:0\n",
      "Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sample 0...\n",
      "\n",
      "Prompt:\n",
      "Write a 300+ word summary of the wikipedia page \"https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli\". Do not use any commas and highlight at least 3 sections that has titles in markdown forma...\n",
      "\n",
      "Generating response...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(test_sample[\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m200\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating response...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m test_response = \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResponse:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(prompt, max_tokens, temperature)\u001b[39m\n\u001b[32m     11\u001b[39m inputs = tokenizer(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m full_text = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m full_text:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\generation\\utils.py:2779\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2777\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2779\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   2780\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   2781\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "TEST_SAMPLE_INDEX = 0\n",
    "\n",
    "print(f\"Testing sample {TEST_SAMPLE_INDEX}...\")\n",
    "test_sample = data[TEST_SAMPLE_INDEX]\n",
    "\n",
    "print(\"\\nPrompt:\")\n",
    "print(test_sample['prompt'][:200] + \"...\")\n",
    "\n",
    "print(\"\\nGenerating response...\")\n",
    "test_response = generate_response(test_sample['prompt'])\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(\"=\" * 80)\n",
    "print(test_response)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nConstraints:\")\n",
    "for i, c in enumerate(test_sample['instruction_id_list']):\n",
    "    print(f\"  {i+1}. {c}\")\n",
    "    print(f\"     {test_sample['kwargs'][i]}\")\n",
    "\n",
    "print(\"\\nEvaluating...\")\n",
    "test_eval = evaluate_single(\n",
    "    test_response,\n",
    "    test_sample['instruction_id_list'],\n",
    "    test_sample['kwargs']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"RESULTS: {test_eval['passed']}/{test_eval['total']} = {test_eval['pass_rate']:.1%}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for r in test_eval['results']:\n",
    "    status = '✓' if r['passed'] else '✗'\n",
    "    print(f\"{status} {r['constraint']}: {r['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running full evaluation...\\n\")\n",
    "\n",
    "all_results = []\n",
    "constraint_stats = defaultdict(list)\n",
    "\n",
    "# Open text file for detailed output\n",
    "txt_output_path = 'instruction_eval_detailed.txt'\n",
    "with open(txt_output_path, 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.write(\"INSTRUCTION FOLLOWING EVALUATION - DETAILED RESULTS\\n\")\n",
    "    txt_file.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for idx in tqdm(range(len(data)), desc=\"Evaluating\"):\n",
    "        sample = data[idx]\n",
    "        \n",
    "        # Generate\n",
    "        response = generate_response(sample['prompt'])\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = evaluate_single(\n",
    "            response,\n",
    "            sample['instruction_id_list'],\n",
    "            sample['kwargs']\n",
    "        )\n",
    "        \n",
    "        # Store\n",
    "        all_results.append({\n",
    "            'idx': idx,\n",
    "            'key': sample.get('key', idx),\n",
    "            'prompt': sample['prompt'],\n",
    "            'response': response,\n",
    "            'eval': eval_result\n",
    "        })\n",
    "        \n",
    "        # Track per-constraint\n",
    "        for r in eval_result['results']:\n",
    "            constraint_stats[r['constraint']].append(r['passed'])\n",
    "        \n",
    "        # Write to text file\n",
    "        txt_file.write(f\"\\nSAMPLE {idx} (Key: {sample.get('key', idx)})\\n\")\n",
    "        txt_file.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        txt_file.write(\"PROMPT:\\n\")\n",
    "        txt_file.write(\"-\" * 80 + \"\\n\")\n",
    "        txt_file.write(sample['prompt'] + \"\\n\\n\")\n",
    "        \n",
    "        txt_file.write(\"MODEL OUTPUT:\\n\")\n",
    "        txt_file.write(\"-\" * 80 + \"\\n\")\n",
    "        txt_file.write(response + \"\\n\\n\")\n",
    "        \n",
    "        txt_file.write(\"EVALUATION:\\n\")\n",
    "        txt_file.write(\"-\" * 80 + \"\\n\")\n",
    "        txt_file.write(f\"Pass Rate: {eval_result['pass_rate']:.1%} ({eval_result['passed']}/{eval_result['total']})\\n\\n\")\n",
    "        \n",
    "        txt_file.write(\"Constraint Results:\\n\")\n",
    "        for r in eval_result['results']:\n",
    "            status = '✓ PASS' if r['passed'] else '✗ FAIL'\n",
    "            txt_file.write(f\"  {status} - {r['constraint']}\\n\")\n",
    "            txt_file.write(f\"           {r['explanation']}\\n\")\n",
    "        \n",
    "        txt_file.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    total_samples = len(all_results)\n",
    "    average_pass_rate = sum(r['eval']['pass_rate'] for r in all_results) / total_samples\n",
    "    perfect = sum(1 for r in all_results if r['eval']['pass_rate'] == 1.0)\n",
    "    partial = sum(1 for r in all_results if 0 < r['eval']['pass_rate'] < 1.0)\n",
    "    failed = sum(1 for r in all_results if r['eval']['pass_rate'] == 0.0)\n",
    "    \n",
    "    # Write summary to text file\n",
    "    txt_file.write(\"\\n\\n\")\n",
    "    txt_file.write(\"=\" * 80 + \"\\n\")\n",
    "    txt_file.write(\"FINAL SUMMARY\\n\")\n",
    "    txt_file.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    txt_file.write(\"Overall Performance:\\n\")\n",
    "    txt_file.write(f\"  Total Samples: {total_samples}\\n\")\n",
    "    txt_file.write(f\"  Average Pass Rate: {average_pass_rate:.1%}\\n\\n\")\n",
    "    \n",
    "    txt_file.write(\"Breakdown:\\n\")\n",
    "    txt_file.write(f\"  Perfect (100%): {perfect} samples\\n\")\n",
    "    txt_file.write(f\"  Partial (>0% <100%): {partial} samples\\n\")\n",
    "    txt_file.write(f\"  Failed (0%): {failed} samples\\n\\n\")\n",
    "    \n",
    "    txt_file.write(\"Per-Constraint Accuracy:\\n\")\n",
    "    txt_file.write(\"-\" * 80 + \"\\n\")\n",
    "    for constraint, passes in sorted(constraint_stats.items()):\n",
    "        accuracy = sum(passes) / len(passes)\n",
    "        txt_file.write(f\"  {constraint}:\\n\")\n",
    "        txt_file.write(f\"    Accuracy: {accuracy:.1%} ({sum(passes)}/{len(passes)})\\n\")\n",
    "    \n",
    "    txt_file.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete!\")\n",
    "print(f\"✓ Detailed results saved to {txt_output_path}\")\n",
    "print(f\"✓ Average Pass Rate: {average_pass_rate:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
