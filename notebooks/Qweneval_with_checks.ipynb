{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Following Evaluation with Check Functions\n",
    "\n",
    "Evaluates model responses using programmatic constraint checkers instead of LLM judge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Hugging Face dataset\n",
    "ds = load_dataset(\"wis-k/instruction-following-eval\")\n",
    "train_ds = ds[\"train\"]\n",
    "data = train_ds.select(range(30))  # Select 30 samples\n",
    "\n",
    "print(f\"Loaded {len(data)} samples\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"Prompt: {data[0]['prompt'][:100]}...\")\n",
    "print(f\"Constraints: {data[0]['instruction_id_list']}\")\n",
    "print(f\"Kwargs: {data[0]['kwargs'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Check Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_punctuation_no_comma(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if response contains no commas.\"\"\"\n",
    "    has_comma = ',' in response\n",
    "    count = response.count(',')\n",
    "    return not has_comma, f\"Commas: {count}\"\n",
    "\n",
    "def check_highlighted_sections(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of highlighted sections using *text* format.\"\"\"\n",
    "    num_required = kwargs.get('num_highlights', 0)\n",
    "    highlights = re.findall(r'\\*[^*]+\\*', response)\n",
    "    count = len(highlights)\n",
    "    return count >= num_required, f\"Highlights: {count}/{num_required}\"\n",
    "\n",
    "def check_number_words(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check word count constraint.\"\"\"\n",
    "    words = len(response.split())\n",
    "    required = kwargs.get('num_words', 0)\n",
    "    relation = kwargs.get('relation', 'at least')\n",
    "    \n",
    "    if relation == 'at least':\n",
    "        passed = words >= required\n",
    "    elif relation == 'less than':\n",
    "        passed = words < required\n",
    "    else:\n",
    "        passed = words == required\n",
    "    \n",
    "    return passed, f\"Words: {words} ({relation} {required})\"\n",
    "\n",
    "def check_number_placeholders(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of placeholders like [address], [name].\"\"\"\n",
    "    num_required = kwargs.get('num_placeholders', 0)\n",
    "    placeholders = re.findall(r'\\[[^\\]]+\\]', response)\n",
    "    count = len(placeholders)\n",
    "    return count >= num_required, f\"Placeholders: {count}/{num_required}\"\n",
    "\n",
    "def check_repeat_prompt(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if prompt is repeated at the beginning.\"\"\"\n",
    "    prompt_to_repeat = kwargs.get('prompt_to_repeat', '')\n",
    "    if not prompt_to_repeat:\n",
    "        return True, \"No prompt to repeat\"\n",
    "    response_start = response.strip()[:len(prompt_to_repeat)]\n",
    "    passed = response_start == prompt_to_repeat.strip()\n",
    "    return passed, f\"Prompt repeated: {passed}\"\n",
    "\n",
    "def check_title_format(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if response contains title in <<title>> format.\"\"\"\n",
    "    has_title = bool(re.search(r'<<[^>]+>>', response))\n",
    "    return has_title, f\"Title found: {has_title}\"\n",
    "\n",
    "def check_english_lowercase(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if entire response is lowercase.\"\"\"\n",
    "    has_uppercase = any(c.isupper() for c in response)\n",
    "    return not has_uppercase, f\"All lowercase: {not has_uppercase}\"\n",
    "\n",
    "def check_number_bullet_lists(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of bullet points using * format.\"\"\"\n",
    "    num_required = kwargs.get('num_bullets', 0)\n",
    "    bullets = re.findall(r'^\\* ', response, re.MULTILINE)\n",
    "    count = len(bullets)\n",
    "    return count == num_required, f\"Bullets: {count}/{num_required}\"\n",
    "\n",
    "def check_english_capital(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if entire response is uppercase.\"\"\"\n",
    "    letters = [c for c in response if c.isalpha()]\n",
    "    if not letters:\n",
    "        return False, \"No letters\"\n",
    "    all_upper = all(c.isupper() for c in letters)\n",
    "    return all_upper, f\"All uppercase: {all_upper}\"\n",
    "\n",
    "def check_multiple_sections(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of sections with splitter.\"\"\"\n",
    "    num_required = kwargs.get('num_sections', 0)\n",
    "    splitter = kwargs.get('section_spliter', 'SECTION')\n",
    "    sections = re.findall(rf'{splitter}\\s*\\d+', response, re.IGNORECASE)\n",
    "    count = len(sections)\n",
    "    return count >= num_required, f\"Sections: {count}/{num_required}\"\n",
    "\n",
    "def check_capital_word_frequency(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check frequency of all-caps words.\"\"\"\n",
    "    capital_frequency = kwargs.get('capital_frequency', 0)\n",
    "    capital_relation = kwargs.get('capital_relation', 'at least')\n",
    "    \n",
    "    words = response.split()\n",
    "    capital_words = [w for w in words if len(w) >= 2 and w.isupper() and w.isalpha()]\n",
    "    count = len(capital_words)\n",
    "    \n",
    "    if capital_relation == 'at least':\n",
    "        passed = count >= capital_frequency\n",
    "    elif capital_relation == 'less than':\n",
    "        passed = count < capital_frequency\n",
    "    else:\n",
    "        passed = count == capital_frequency\n",
    "    \n",
    "    return passed, f\"Capital words: {count} ({capital_relation} {capital_frequency})\"\n",
    "\n",
    "def check_quotation(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if wrapped in quotes.\"\"\"\n",
    "    response_stripped = response.strip()\n",
    "    has_quotes = response_stripped.startswith('\"') and response_stripped.endswith('\"')\n",
    "    return has_quotes, f\"Quoted: {has_quotes}\"\n",
    "\n",
    "def check_keywords_existence(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if keywords exist.\"\"\"\n",
    "    keywords = kwargs.get('keywords', [])\n",
    "    if not keywords:\n",
    "        return True, \"No keywords\"\n",
    "    response_lower = response.lower()\n",
    "    missing = [k for k in keywords if k.lower() not in response_lower]\n",
    "    return len(missing) == 0, f\"Missing: {missing}\" if missing else \"All present\"\n",
    "\n",
    "def check_json_format(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check if valid JSON.\"\"\"\n",
    "    try:\n",
    "        response_stripped = response.strip()\n",
    "        if response_stripped.startswith('```'):\n",
    "            response_stripped = re.sub(r'^```(?:json)?\\s*', '', response_stripped)\n",
    "            response_stripped = re.sub(r'```\\s*$', '', response_stripped)\n",
    "        json.loads(response_stripped)\n",
    "        return True, \"Valid JSON\"\n",
    "    except:\n",
    "        return False, \"Invalid JSON\"\n",
    "\n",
    "def check_number_paragraphs(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check number of paragraphs.\"\"\"\n",
    "    num_required = kwargs.get('num_paragraphs', 0)\n",
    "    relation = kwargs.get('relation', 'at least')\n",
    "    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n|\\*\\*\\*', response) if p.strip()]\n",
    "    count = len(paragraphs)\n",
    "    \n",
    "    if relation == 'at least':\n",
    "        passed = count >= num_required\n",
    "    else:\n",
    "        passed = count == num_required\n",
    "    \n",
    "    return passed, f\"Paragraphs: {count} ({relation} {num_required})\"\n",
    "\n",
    "def check_two_responses(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check for two parts separated by ******.\"\"\"\n",
    "    parts = response.split('******')\n",
    "    return len(parts) >= 2, f\"Parts: {len(parts)}/2\"\n",
    "\n",
    "def check_response_language(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Language check (simplified).\"\"\"\n",
    "    lang = kwargs.get('language', 'en')\n",
    "    return len(response.strip()) > 0, f\"Lang check ({lang})\"\n",
    "\n",
    "def check_letter_frequency(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check letter frequency.\"\"\"\n",
    "    letter = kwargs.get('letter', '')\n",
    "    frequency = kwargs.get('let_frequency', 0)\n",
    "    relation = kwargs.get('let_relation', 'at least')\n",
    "    \n",
    "    if not letter:\n",
    "        return True, \"No letter\"\n",
    "    \n",
    "    count = response.count(letter)\n",
    "    \n",
    "    if relation == 'at least':\n",
    "        passed = count >= frequency\n",
    "    elif relation == 'less than':\n",
    "        passed = count < frequency\n",
    "    else:\n",
    "        passed = count == frequency\n",
    "    \n",
    "    return passed, f\"'{letter}': {count} ({relation} {frequency})\"\n",
    "\n",
    "def check_end_checker(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check ending phrase.\"\"\"\n",
    "    end_phrase = kwargs.get('end_phrase', '')\n",
    "    if not end_phrase:\n",
    "        return True, \"No end phrase\"\n",
    "    passed = response.strip().endswith(end_phrase)\n",
    "    return passed, f\"Ends correctly: {passed}\"\n",
    "\n",
    "def check_forbidden_words(response: str, kwargs: Dict) -> tuple:\n",
    "    \"\"\"Check forbidden words.\"\"\"\n",
    "    forbidden = kwargs.get('forbidden_words', [])\n",
    "    if not forbidden:\n",
    "        return True, \"No forbidden words\"\n",
    "    response_lower = response.lower()\n",
    "    found = [w for w in forbidden if w.lower() in response_lower]\n",
    "    return len(found) == 0, f\"Forbidden: {found}\" if found else \"None\"\n",
    "\n",
    "# Mapping\n",
    "CONSTRAINT_CHECKERS = {\n",
    "    'punctuation:no_comma': check_punctuation_no_comma,\n",
    "    'detectable_format:number_highlighted_sections': check_highlighted_sections,\n",
    "    'length_constraints:number_words': check_number_words,\n",
    "    'detectable_content:number_placeholders': check_number_placeholders,\n",
    "    'combination:repeat_prompt': check_repeat_prompt,\n",
    "    'detectable_format:title': check_title_format,\n",
    "    'change_case:english_lowercase': check_english_lowercase,\n",
    "    'detectable_format:number_bullet_lists': check_number_bullet_lists,\n",
    "    'change_case:english_capital': check_english_capital,\n",
    "    'detectable_format:multiple_sections': check_multiple_sections,\n",
    "    'change_case:capital_word_frequency': check_capital_word_frequency,\n",
    "    'startend:quotation': check_quotation,\n",
    "    'keywords:existence': check_keywords_existence,\n",
    "    'detectable_format:json_format': check_json_format,\n",
    "    'length_constraints:number_paragraphs': check_number_paragraphs,\n",
    "    'combination:two_responses': check_two_responses,\n",
    "    'language:response_language': check_response_language,\n",
    "    'keywords:letter_frequency': check_letter_frequency,\n",
    "    'startend:end_checker': check_end_checker,\n",
    "    'keywords:forbidden_words': check_forbidden_words,\n",
    "}\n",
    "\n",
    "print(f\"✓ Loaded {len(CONSTRAINT_CHECKERS)} check functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single(response: str, instruction_id_list: List[str], kwargs_list: List[Dict]) -> Dict:\n",
    "    \"\"\"Evaluate response with check functions.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, constraint_type in enumerate(instruction_id_list):\n",
    "        kwargs = kwargs_list[i] if i < len(kwargs_list) else {}\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        \n",
    "        check_func = CONSTRAINT_CHECKERS.get(constraint_type)\n",
    "        \n",
    "        if check_func:\n",
    "            try:\n",
    "                passed, explanation = check_func(response, kwargs)\n",
    "            except Exception as e:\n",
    "                passed = False\n",
    "                explanation = f\"Error: {str(e)}\"\n",
    "        else:\n",
    "            passed = False\n",
    "            explanation = f\"Unknown: {constraint_type}\"\n",
    "        \n",
    "        results.append({\n",
    "            'constraint': constraint_type,\n",
    "            'passed': passed,\n",
    "            'explanation': explanation\n",
    "        })\n",
    "    \n",
    "    total = len(results)\n",
    "    passed_count = sum(r['passed'] for r in results)\n",
    "    pass_rate = passed_count / total if total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'passed': passed_count,\n",
    "        'total': total,\n",
    "        'pass_rate': pass_rate\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_PATH}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"✓ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Response Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_tokens: int = 1024, temperature: float = 0.2) -> str:\n",
    "    \"\"\"Generate response using chat template.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"assistant\" in full_text:\n",
    "        response = full_text.split(\"assistant\")[-1].strip()\n",
    "    else:\n",
    "        gen_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        response = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"✓ Generation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SAMPLE_INDEX = 0\n",
    "\n",
    "print(f\"Testing sample {TEST_SAMPLE_INDEX}...\")\n",
    "test_sample = data[TEST_SAMPLE_INDEX]\n",
    "\n",
    "print(\"\\nPrompt:\")\n",
    "print(test_sample['prompt'][:200] + \"...\")\n",
    "\n",
    "print(\"\\nGenerating response...\")\n",
    "test_response = generate_response(test_sample['prompt'])\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(\"=\" * 80)\n",
    "print(test_response)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nConstraints:\")\n",
    "for i, c in enumerate(test_sample['instruction_id_list']):\n",
    "    print(f\"  {i+1}. {c}\")\n",
    "    print(f\"     {test_sample['kwargs'][i]}\")\n",
    "\n",
    "print(\"\\nEvaluating...\")\n",
    "test_eval = evaluate_single(\n",
    "    test_response,\n",
    "    test_sample['instruction_id_list'],\n",
    "    test_sample['kwargs']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"RESULTS: {test_eval['passed']}/{test_eval['total']} = {test_eval['pass_rate']:.1%}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for r in test_eval['results']:\n",
    "    status = '✓' if r['passed'] else '✗'\n",
    "    print(f\"{status} {r['constraint']}: {r['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running full evaluation...\\n\")\n",
    "\n",
    "all_results = []\n",
    "constraint_stats = defaultdict(list)\n",
    "\n",
    "for idx in tqdm(range(len(data)), desc=\"Evaluating\"):\n",
    "    sample = data[idx]\n",
    "    \n",
    "    # Generate\n",
    "    response = generate_response(sample['prompt'])\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_result = evaluate_single(\n",
    "        response,\n",
    "        sample['instruction_id_list'],\n",
    "        sample['kwargs']\n",
    "    )\n",
    "    \n",
    "    # Store\n",
    "    all_results.append({\n",
    "        'idx': idx,\n",
    "        'key': sample.get('key', idx),\n",
    "        'prompt': sample['prompt'],\n",
    "        'response': response,\n",
    "        'eval': eval_result\n",
    "    })\n",
    "    \n",
    "    # Track per-constraint\n",
    "    for r in eval_result['results']:\n",
    "        constraint_stats[r['constraint']].append(r['passed'])\n",
    "\n",
    "print(\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "total_samples = len(all_results)\n",
    "average_pass_rate = sum(r['eval']['pass_rate'] for r in all_results) / total_samples\n",
    "\n",
    "perfect = sum(1 for r in all_results if r['eval']['pass_rate'] == 1.0)\n",
    "partial = sum(1 for r in all_results if 0 < r['eval']['pass_rate'] < 1.0)\n",
    "failed = sum(1 for r in all_results if r['eval']['pass_rate'] == 0.0)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal Samples: {total_samples}\")\n",
    "print(f\"Average Pass Rate: {average_pass_rate:.1%}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  Perfect (100%): {perfect}\")\n",
    "print(f\"  Partial: {partial}\")\n",
    "print(f\"  Failed (0%): {failed}\")\n",
    "\n",
    "print(f\"\\nPer-Constraint Accuracy:\")\n",
    "for constraint, passes in sorted(constraint_stats.items()):\n",
    "    accuracy = sum(passes) / len(passes)\n",
    "    print(f\"  {constraint}:\")\n",
    "    print(f\"    {accuracy:.1%} ({sum(passes)}/{len(passes)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample Results (First 3):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for result in all_results[:3]:\n",
    "    print(f\"\\n[Sample {result['idx']}]\")\n",
    "    print(f\"Prompt: {result['prompt'][:80]}...\")\n",
    "    print(f\"Pass Rate: {result['eval']['pass_rate']:.1%}\")\n",
    "    \n",
    "    for r in result['eval']['results']:\n",
    "        status = '✓' if r['passed'] else '✗'\n",
    "        print(f\"  {status} {r['constraint']}: {r['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "output = {\n",
    "    'model': MODEL_PATH,\n",
    "    'total_samples': total_samples,\n",
    "    'average_pass_rate': average_pass_rate,\n",
    "    'breakdown': {\n",
    "        'perfect': perfect,\n",
    "        'partial': partial,\n",
    "        'failed': failed\n",
    "    },\n",
    "    'constraint_accuracy': {\n",
    "        k: {\n",
    "            'accuracy': sum(v) / len(v),\n",
    "            'passed': sum(v),\n",
    "            'total': len(v)\n",
    "        } for k, v in constraint_stats.items()\n",
    "    },\n",
    "    'all_results': all_results\n",
    "}\n",
    "\n",
    "output_path = 'instruction_eval_results.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Results saved to {output_path}\")\n",
    "print(f\"\\nFinal Average Pass Rate: {average_pass_rate:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
