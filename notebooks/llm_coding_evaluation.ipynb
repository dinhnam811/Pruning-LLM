{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Coding Capability Evaluation\n",
    "\n",
    "This notebook evaluates the coding capabilities of LLMs using two metrics:\n",
    "1. **Exact Match (EM)**: Token-by-token comparison with reference implementation\n",
    "2. **Pass@k**: Whether at least one of k generations passes all unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load JSONL dataset.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load the evaluation dataset\n",
    "dataset_path = '../data/eval/code_samples_full.jsonl'\n",
    "dataset = load_dataset(dataset_path)\n",
    "print(f\"Loaded {len(dataset)} samples\")\n",
    "print(f\"\\nExample sample:\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your model path here\n",
    "MODEL_PATH = \"Qwen/Qwen2.5-Coder-3B-Instruct\"  # Update this with your actual model path\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_prompt(sample: Dict[str, Any]) -> str:\n    \"\"\"Create a prompt for code generation.\"\"\"\n    language = sample['language']\n    func_name = sample['func_name']\n    docstring = sample['docstring']\n    \n    # Strict prompt to get clean Java code\n    prompt = f\"\"\"You are a Java code generator.\n\nWrite ONLY the Java method for this task. Follow these rules:\n- Output MUST start with 'public static'\n- Output MUST be a complete method\n- NO explanations, NO comments, NO markdown\n\nTask: {docstring}\nFunction name: {func_name}\n\nWrite the complete Java method:\"\"\"\n    return prompt\n\ndef clean_generated_code(raw_output: str) -> str:\n    \"\"\"Clean and extract the method from raw model output.\"\"\"\n    code = raw_output.strip()\n    \n    # Remove markdown code blocks\n    code = re.sub(r'```java\\s*', '', code)\n    code = re.sub(r'```\\s*', '', code)\n    \n    # Remove any explanatory text before the code\n    if 'public static' in code:\n        code = code[code.index('public static'):]\n    \n    # Extract just the first method (find matching braces)\n    if 'public static' in code:\n        # Count braces to find the complete method\n        brace_count = 0\n        in_method = False\n        method_chars = []\n        \n        for char in code:\n            if char == '{':\n                brace_count += 1\n                in_method = True\n            if in_method:\n                method_chars.append(char)\n            if char == '}':\n                brace_count -= 1\n                if in_method and brace_count == 0:\n                    break\n        \n        if method_chars:\n            code = ''.join(method_chars)\n            # Add back the signature before the opening brace\n            if '{' in code:\n                sig_end = code.index('{')\n                # Find signature from original code\n                sig_match = re.search(r'public\\s+static\\s+[^{]+', raw_output)\n                if sig_match:\n                    signature = sig_match.group(0).strip()\n                    code = signature + ' ' + code[sig_end:]\n    \n    # Alternative: simple extraction with rfind for closing brace\n    if 'public static' in code and '}' in code:\n        # Find last closing brace\n        last_brace = code.rfind('}')\n        code = code[:last_brace + 1]\n    \n    return code.strip()\n\ndef generate_code(prompt: str, num_samples: int = 1, max_length: int = 256, temperature: float = 0.2, top_p: float = 0.95) -> List[str]:\n    \"\"\"Generate code completions from the model.\n    \n    Args:\n        prompt: The input prompt\n        num_samples: Number of completions to generate (for Pass@k)\n        max_length: Maximum length of generated tokens\n        temperature: Sampling temperature (default 0.2 for more focused output)\n        top_p: Nucleus sampling parameter\n    \n    Returns:\n        List of cleaned generated code strings\n    \"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate multiple samples\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_length,\n        num_return_sequences=num_samples,\n        temperature=temperature if num_samples > 1 else 0.1,\n        top_p=top_p,\n        do_sample=num_samples > 1,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    \n    # Decode and clean outputs\n    generated_texts = []\n    for output in outputs:\n        # Decode full output\n        full_text = tokenizer.decode(output, skip_special_tokens=True)\n        \n        # Remove the prompt\n        if prompt in full_text:\n            generated_text = full_text[len(prompt):].strip()\n        else:\n            # Fallback: decode only new tokens\n            gen_ids = output[inputs[\"input_ids\"].shape[1]:]\n            generated_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n        \n        # Clean and extract the method\n        cleaned_code = clean_generated_code(generated_text)\n        generated_texts.append(cleaned_code)\n    \n    return generated_texts\n\n# Test code generation\ntest_sample = dataset[0]\ntest_prompt = create_prompt(test_sample)\nprint(\"Test prompt:\")\nprint(test_prompt)\nprint(\"\\nGenerating code...\")\ntest_output = generate_code(test_prompt, num_samples=1)\nprint(\"\\nGenerated code:\")\nprint(test_output[0])\nprint(\"\\nReference code:\")\nprint(test_sample['code'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics\n",
    "\n",
    "### 4.1 Exact Match (EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_code(code: str) -> str:\n",
    "    \"\"\"Normalize code by removing extra whitespace.\"\"\"\n",
    "    # Remove leading/trailing whitespace\n",
    "    code = code.strip()\n",
    "    # Normalize internal whitespace (optional - can make matching more lenient)\n",
    "    # code = ' '.join(code.split())\n",
    "    return code\n",
    "\n",
    "def exact_match(generated_code: str, reference_code: str) -> int:\n",
    "    \"\"\"Calculate exact match score (1 if match, 0 otherwise).\n",
    "    \n",
    "    Args:\n",
    "        generated_code: Code generated by the model\n",
    "        reference_code: Ground truth reference code\n",
    "    \n",
    "    Returns:\n",
    "        1 if exact match, 0 otherwise\n",
    "    \"\"\"\n",
    "    gen_normalized = normalize_code(generated_code)\n",
    "    ref_normalized = normalize_code(reference_code)\n",
    "    \n",
    "    return 1 if gen_normalized == ref_normalized else 0\n",
    "\n",
    "# Test exact match\n",
    "print(\"Testing Exact Match metric:\")\n",
    "print(f\"Match: {exact_match(test_sample['code'], test_sample['code'])}\")\n",
    "print(f\"No match: {exact_match('different code', test_sample['code'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pass@k Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_code(generated_text: str, func_name: str) -> str:\n",
    "    \"\"\"Extract the function code from generated text.\"\"\"\n",
    "    # Try to find the function definition\n",
    "    # This is a simple extraction - you may need to customize based on your LLM's output format\n",
    "    lines = generated_text.strip().split('\\n')\n",
    "    code_lines = []\n",
    "    in_function = False\n",
    "    \n",
    "    for line in lines:\n",
    "        if func_name in line and ('public' in line or 'private' in line or 'static' in line):\n",
    "            in_function = True\n",
    "        if in_function:\n",
    "            code_lines.append(line)\n",
    "            # Simple heuristic: if we find a closing brace at the start of line, function might end\n",
    "            if line.strip() == '}':\n",
    "                break\n",
    "    \n",
    "    return '\\n'.join(code_lines) if code_lines else generated_text\n",
    "\n",
    "def run_tests(code: str, test_cases: List[Dict]) -> bool:\n",
    "    \"\"\"Run test cases against generated code.\n",
    "    \n",
    "    Note: This is a placeholder. For Java code, you would need to:\n",
    "    1. Write the code to a .java file\n",
    "    2. Compile it\n",
    "    3. Run test cases\n",
    "    4. Check if all tests pass\n",
    "    \n",
    "    For now, we'll use exact match as a proxy for passing tests.\n",
    "    \"\"\"\n",
    "    # Placeholder implementation\n",
    "    # In a real scenario, you would compile and execute the code\n",
    "    return True  # Placeholder\n",
    "\n",
    "def calculate_pass_at_k(completions: List[str], reference_code: str, k: int = 1) -> float:\n",
    "    \"\"\"Calculate Pass@k metric.\n",
    "    \n",
    "    Args:\n",
    "        completions: List of generated code completions\n",
    "        reference_code: Ground truth reference code\n",
    "        k: Number of completions to consider\n",
    "    \n",
    "    Returns:\n",
    "        1.0 if at least one of the top-k completions passes, 0.0 otherwise\n",
    "    \"\"\"\n",
    "    # Limit to k completions\n",
    "    completions_to_check = completions[:k]\n",
    "    \n",
    "    # For this implementation, we'll use exact match as a proxy for \"passing tests\"\n",
    "    # In a real scenario, you would run actual unit tests\n",
    "    for completion in completions_to_check:\n",
    "        if exact_match(completion, reference_code):\n",
    "            return 1.0\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "# Test Pass@k\n",
    "print(\"Testing Pass@k metric:\")\n",
    "test_completions = [\"wrong code\", test_sample['code'], \"another wrong code\"]\n",
    "print(f\"Pass@1: {calculate_pass_at_k(test_completions, test_sample['code'], k=1)}\")\n",
    "print(f\"Pass@3: {calculate_pass_at_k(test_completions, test_sample['code'], k=3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset: List[Dict], k_values: List[int] = [1, 5, 10], num_samples: int = 10):\n",
    "    \"\"\"Run complete evaluation on the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: List of evaluation samples\n",
    "        k_values: List of k values for Pass@k metric\n",
    "        num_samples: Number of completions to generate per problem (should be >= max(k_values))\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'exact_match': [],\n",
    "        'pass_at_k': {k: [] for k in k_values},\n",
    "        'samples': []\n",
    "    }\n",
    "    \n",
    "    for idx, sample in enumerate(tqdm(dataset, desc=\"Evaluating\")):\n",
    "        # Create prompt\n",
    "        prompt = create_prompt(sample)\n",
    "        \n",
    "        # Generate completions\n",
    "        completions = generate_code(prompt, num_samples=num_samples)\n",
    "        \n",
    "        # Calculate Exact Match for the first generation\n",
    "        em_score = exact_match(completions[0], sample['code'])\n",
    "        results['exact_match'].append(em_score)\n",
    "        \n",
    "        # Calculate Pass@k for different k values\n",
    "        for k in k_values:\n",
    "            pass_k_score = calculate_pass_at_k(completions, sample['code'], k=k)\n",
    "            results['pass_at_k'][k].append(pass_k_score)\n",
    "        \n",
    "        # Store sample results\n",
    "        results['samples'].append({\n",
    "            'idx': idx,\n",
    "            'func_name': sample['func_name'],\n",
    "            'reference': sample['code'],\n",
    "            'generated': completions[0],\n",
    "            'all_completions': completions,\n",
    "            'exact_match': em_score\n",
    "        })\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    results['aggregate'] = {\n",
    "        'exact_match': np.mean(results['exact_match']),\n",
    "        'pass_at_k': {k: np.mean(results['pass_at_k'][k]) for k in k_values}\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "# Note: Adjust num_samples based on your computational resources\n",
    "# For quick testing, use a subset of the dataset\n",
    "\n",
    "# Option 1: Evaluate on full dataset\n",
    "eval_results = evaluate_model(dataset, k_values=[1, 5, 10], num_samples=10)\n",
    "\n",
    "# Option 2: Evaluate on a small subset for testing\n",
    "# eval_results = evaluate_model(dataset[:5], k_values=[1, 5, 10], num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print aggregate results\n",
    "print(\"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nDataset size: {len(dataset)} samples\")\n",
    "print(f\"\\nExact Match (EM): {eval_results['aggregate']['exact_match']:.2%}\")\n",
    "print(f\"\\nPass@k Scores:\")\n",
    "for k, score in eval_results['aggregate']['pass_at_k'].items():\n",
    "    print(f\"  Pass@{k}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example comparisons\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE COMPARISONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "num_examples = min(3, len(eval_results['samples']))\n",
    "for i in range(num_examples):\n",
    "    sample = eval_results['samples'][i]\n",
    "    print(f\"\\nExample {i+1}: {sample['func_name']}\")\n",
    "    print(f\"Exact Match: {'✓' if sample['exact_match'] else '✗'}\")\n",
    "    print(f\"\\nReference Code:\\n{sample['reference']}\")\n",
    "    print(f\"\\nGenerated Code:\\n{sample['generated']}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Pass@k scores\n",
    "k_values = list(eval_results['aggregate']['pass_at_k'].keys())\n",
    "pass_k_scores = [eval_results['aggregate']['pass_at_k'][k] for k in k_values]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Pass@k plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar([f\"Pass@{k}\" for k in k_values], pass_k_scores, color='skyblue')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Pass@k Performance')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(pass_k_scores):\n",
    "    plt.text(i, v + 0.02, f\"{v:.2%}\", ha='center')\n",
    "\n",
    "# EM score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Exact Match'], [eval_results['aggregate']['exact_match']], color='lightcoral')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Exact Match Performance')\n",
    "plt.ylim(0, 1)\n",
    "plt.text(0, eval_results['aggregate']['exact_match'] + 0.02, \n",
    "         f\"{eval_results['aggregate']['exact_match']:.2%}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to JSON\n",
    "output_path = '../data/eval/evaluation_results.json'\n",
    "\n",
    "# Prepare results for JSON serialization\n",
    "json_results = {\n",
    "    'model_path': MODEL_PATH,\n",
    "    'dataset_size': len(dataset),\n",
    "    'aggregate_metrics': eval_results['aggregate'],\n",
    "    'detailed_samples': eval_results['samples'][:10]  # Save first 10 samples\n",
    "}\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis by Function Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by function name\n",
    "from collections import defaultdict\n",
    "\n",
    "performance_by_func = defaultdict(list)\n",
    "for i, sample_result in enumerate(eval_results['samples']):\n",
    "    func_name = dataset[i]['func_name']\n",
    "    performance_by_func[func_name].append(sample_result['exact_match'])\n",
    "\n",
    "print(\"Performance by Function Type:\")\n",
    "print(\"=\"*50)\n",
    "for func_name, scores in sorted(performance_by_func.items()):\n",
    "    avg_score = np.mean(scores)\n",
    "    count = len(scores)\n",
    "    print(f\"{func_name:20s}: {avg_score:.2%} ({count} samples)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}