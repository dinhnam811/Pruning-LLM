{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Original vs Pruned Model\n",
    "This notebook compares the original Qwen2.5-Coder-3B-Instruct with your pruned model.\n",
    "\n",
    "We will test both models on 20 Java coding problems and compare:\n",
    "- **Speed**: How fast each model generates code\n",
    "- **Accuracy**: How many problems each model solves correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Problems\n",
    "We'll use the first 20 problems from the Java benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 test problems\n"
     ]
    }
   ],
   "source": [
    "# Simple test problems (first 20)\n",
    "benchmark = [\n",
    "  {\"id\": \"java_001_is_prime\", \"prompt\": \"Write a Java method isPrime that returns true if n is a prime number, otherwise false.\", \"signature\": \"public static boolean isPrime(int n)\", \"tests\": [{\"input\": \"2\", \"expected\": \"true\"}, {\"input\": \"4\", \"expected\": \"false\"}, {\"input\": \"17\", \"expected\": \"true\"}]},\n",
    "  {\"id\": \"java_002_reverse_string\", \"prompt\": \"Write a Java method reverseString that returns the reversed version of the input string.\", \"signature\": \"public static String reverseString(String s)\", \"tests\": [{\"input\": \"\\\"abc\\\"\", \"expected\": \"\\\"cba\\\"\"}, {\"input\": \"\\\"hello\\\"\", \"expected\": \"\\\"olleh\\\"\"}]},\n",
    "  {\"id\": \"java_003_sum_array\", \"prompt\": \"Write a Java method sumArray that returns the sum of all elements in the given integer array.\", \"signature\": \"public static int sumArray(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"6\"}, {\"input\": \"new int[]{0, 0, 0}\", \"expected\": \"0\"}]},\n",
    "  {\"id\": \"java_004_factorial\", \"prompt\": \"Write a Java method factorial that returns n! (n factorial). Assume n is non-negative.\", \"signature\": \"public static long factorial(int n)\", \"tests\": [{\"input\": \"0\", \"expected\": \"1\"}, {\"input\": \"5\", \"expected\": \"120\"}]},\n",
    "  {\"id\": \"java_005_max_in_array\", \"prompt\": \"Write a Java method maxInArray that returns the maximum value in the given integer array.\", \"signature\": \"public static int maxInArray(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"3\"}, {\"input\": \"new int[]{-1, -5, -3}\", \"expected\": \"-1\"}]},\n",
    "  {\"id\": \"java_006_min_in_array\", \"prompt\": \"Write a Java method minInArray that returns the minimum value in the given integer array.\", \"signature\": \"public static int minInArray(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"1\"}, {\"input\": \"new int[]{-1, -5, -3}\", \"expected\": \"-5\"}]},\n",
    "  {\"id\": \"java_007_is_palindrome\", \"prompt\": \"Write a Java method isPalindrome that returns true if the given string is a palindrome.\", \"signature\": \"public static boolean isPalindrome(String s)\", \"tests\": [{\"input\": \"\\\"racecar\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"abc\\\"\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_008_count_vowels\", \"prompt\": \"Write a Java method countVowels that returns the number of vowels in the given string.\", \"signature\": \"public static int countVowels(String s)\", \"tests\": [{\"input\": \"\\\"hello\\\"\", \"expected\": \"2\"}, {\"input\": \"\\\"AEIOU\\\"\", \"expected\": \"5\"}]},\n",
    "  {\"id\": \"java_009_fibonacci\", \"prompt\": \"Write a Java method fibonacci that returns the n-th Fibonacci number.\", \"signature\": \"public static int fibonacci(int n)\", \"tests\": [{\"input\": \"0\", \"expected\": \"0\"}, {\"input\": \"5\", \"expected\": \"5\"}]},\n",
    "  {\"id\": \"java_010_find_index\", \"prompt\": \"Write a Java method findIndex that returns the index of target in the array, or -1 if not found.\", \"signature\": \"public static int findIndex(int[] arr, int target)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3, 4}, 3\", \"expected\": \"2\"}, {\"input\": \"new int[]{1, 2, 3, 4}, 5\", \"expected\": \"-1\"}]},\n",
    "  {\"id\": \"java_011_contains_duplicate\", \"prompt\": \"Write a Java method containsDuplicate that returns true if any value appears at least twice in the array.\", \"signature\": \"public static boolean containsDuplicate(int[] nums)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3, 1}\", \"expected\": \"true\"}, {\"input\": \"new int[]{1, 2, 3, 4}\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_012_max_subarray_sum\", \"prompt\": \"Write a Java method maxSubArraySum that returns the largest sum of a contiguous subarray.\", \"signature\": \"public static int maxSubArraySum(int[] nums)\", \"tests\": [{\"input\": \"new int[]{-2,1,-3,4,-1,2,1,-5,4}\", \"expected\": \"6\"}, {\"input\": \"new int[]{1}\", \"expected\": \"1\"}]},\n",
    "  {\"id\": \"java_013_two_sum\", \"prompt\": \"Write a Java method hasTwoSum that returns true if there exist two distinct indices i and j such that nums[i] + nums[j] == target.\", \"signature\": \"public static boolean hasTwoSum(int[] nums, int target)\", \"tests\": [{\"input\": \"new int[]{2, 7, 11, 15}, 9\", \"expected\": \"true\"}, {\"input\": \"new int[]{1, 2, 3}, 10\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_014_is_anagram\", \"prompt\": \"Write a Java method isAnagram that returns true if two given strings are anagrams of each other.\", \"signature\": \"public static boolean isAnagram(String s, String t)\", \"tests\": [{\"input\": \"\\\"anagram\\\", \\\"nagaram\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"rat\\\", \\\"car\\\"\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_015_remove_whitespace\", \"prompt\": \"Write a Java method removeWhitespace that returns a new string with all whitespace removed.\", \"signature\": \"public static String removeWhitespace(String s)\", \"tests\": [{\"input\": \"\\\"a b c\\\"\", \"expected\": \"\\\"abc\\\"\"}, {\"input\": \"\\\"   hello   world   \\\"\", \"expected\": \"\\\"helloworld\\\"\"}]},\n",
    "  {\"id\": \"java_016_power\", \"prompt\": \"Write a Java method power that returns x raised to the power n (x^n).\", \"signature\": \"public static long power(int x, int n)\", \"tests\": [{\"input\": \"2, 3\", \"expected\": \"8\"}, {\"input\": \"5, 2\", \"expected\": \"25\"}]},\n",
    "  {\"id\": \"java_017_is_sorted\", \"prompt\": \"Write a Java method isSortedAscending that returns true if the array is sorted in ascending order.\", \"signature\": \"public static boolean isSortedAscending(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3, 4}\", \"expected\": \"true\"}, {\"input\": \"new int[]{3, 2, 1}\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_018_second_largest\", \"prompt\": \"Write a Java method secondLargest that returns the second largest distinct number in the array.\", \"signature\": \"public static int secondLargest(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"2\"}, {\"input\": \"new int[]{5, 1, 5, 2}\", \"expected\": \"2\"}]},\n",
    "  {\"id\": \"java_019_is_rotation\", \"prompt\": \"Write a Java method isRotation that returns true if string b is a rotation of string a.\", \"signature\": \"public static boolean isRotation(String a, String b)\", \"tests\": [{\"input\": \"\\\"abcde\\\", \\\"cdeab\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"abcde\\\", \\\"abced\\\"\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_020_valid_parentheses\", \"prompt\": \"Write a Java method isValidParentheses that returns true if the input string containing brackets is valid.\", \"signature\": \"public static boolean isValidParentheses(String s)\", \"tests\": [{\"input\": \"\\\"()\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"([)]\\\"\", \"expected\": \"false\"}]}\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(benchmark)} test problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models\n",
    "\n",
    "**IMPORTANT**: Update the `PRUNED_MODEL_PATH` to point to your pruned model folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading ORIGINAL model from HuggingFace...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead43fc658d24e8b801e5fbf303c4916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Original model loaded on cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load original model\n",
    "original_model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "print(\" Loading ORIGINAL model from HuggingFace...\")\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(original_model_name, trust_remote_code=True)\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    original_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"✓ Original model loaded on {original_model.device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PRUNED model from local path...\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '..\\Qwen-Coder-Prunned-New'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars, '-', '_' or '.'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '..\\Qwen-Coder-Prunned-New'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1378965980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading PRUNED model from local path...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m pruned_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    509\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         resolved_files = [\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         ]\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    141\u001b[0m ):\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars, '-', '_' or '.'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34m\" The name cannot start or end with '-' or '.' and the maximum length is 96:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '..\\Qwen-Coder-Prunned-New'."
     ]
    }
   ],
   "source": [
    "# Load pruned model\n",
    "\n",
    "model_path = r\"..\\Qwen-Coder-Prunned-New\"\n",
    "\n",
    "print(\"Loading PRUNED model from local path...\")\n",
    "\n",
    "pruned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "print(f\"✓ Pruned model loaded on {pruned_model.device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "These functions help us generate code and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(model, tokenizer, task):\n",
    "    \"\"\"\n",
    "    Generate Java code for a given task using a model.\n",
    "    Returns: (generated_code, time_taken_in_seconds)\n",
    "    \"\"\"\n",
    "    signature = task[\"signature\"]\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Write ONLY the Java method for this task:\n",
    "{task['prompt']}\n",
    "\n",
    "Signature: {signature}\n",
    "\n",
    "Write the complete method:\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate and measure time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.2,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    code = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Clean up the output\n",
    "    code = code.replace(\"```java\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    # Extract just the method\n",
    "    if \"public static\" in code:\n",
    "        code = code[code.index(\"public static\"):]\n",
    "        if \"}\" in code:\n",
    "            code = code[:code.rfind(\"}\")+1]\n",
    "    \n",
    "    return code, elapsed_time\n",
    "\n",
    "\n",
    "def test_code(task, method_code):\n",
    "    \"\"\"\n",
    "    Test if the generated code passes all test cases.\n",
    "    Returns: True if all tests pass, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse signature to get return type and method name\n",
    "        sig_match = re.search(r'public\\s+static\\s+(\\S+)\\s+([A-Za-z_][A-Za-z0-9_]*)\\s*\\(', task[\"signature\"])\n",
    "        if not sig_match:\n",
    "            return False\n",
    "        \n",
    "        return_type, method_name = sig_match.group(1), sig_match.group(2)\n",
    "        \n",
    "        # Build test code\n",
    "        test_calls = []\n",
    "        for i, test in enumerate(task[\"tests\"], 1):\n",
    "            inp, expected = test[\"input\"], test[\"expected\"]\n",
    "            \n",
    "            if return_type == \"String\":\n",
    "                condition = f\"!{method_name}({inp}).equals({expected})\"\n",
    "            else:\n",
    "                condition = f\"{method_name}({inp}) != {expected}\"\n",
    "            \n",
    "            test_calls.append(f\"if ({condition}) throw new Exception(\\\"Test {i} failed\\\");\")\n",
    "        \n",
    "        # Create full Java file\n",
    "        java_code = f\"\"\"\n",
    "public class Main {{\n",
    "    {method_code}\n",
    "    \n",
    "    public static void main(String[] args) {{\n",
    "        try {{\n",
    "            {chr(10).join('            ' + tc for tc in test_calls)}\n",
    "            System.out.println(\"OK\");\n",
    "        }} catch (Exception e) {{\n",
    "            System.out.println(\"FAIL\");\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        # Write to file\n",
    "        os.makedirs(\"temp\", exist_ok=True)\n",
    "        with open(\"temp/Main.java\", \"w\") as f:\n",
    "            f.write(java_code)\n",
    "        \n",
    "        # Compile\n",
    "        compile_result = subprocess.run([\"javac\", \"temp/Main.java\"], capture_output=True, text=True)\n",
    "        if compile_result.returncode != 0:\n",
    "            return False\n",
    "        \n",
    "        # Run\n",
    "        run_result = subprocess.run([\"java\", \"-cp\", \"temp\", \"Main\"], capture_output=True, text=True, timeout=5)\n",
    "        return run_result.stdout.strip() == \"OK\"\n",
    "        \n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Comparison\n",
    "Test both models on all problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "original_results = []\n",
    "pruned_results = []\n",
    "\n",
    "print(\"Starting comparison...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, task in enumerate(benchmark, 1):\n",
    "    print(f\"\\n[{i}/{len(benchmark)}] Testing: {task['id']}\")\n",
    "    \n",
    "    # Test original model\n",
    "    print(\"  → Original model...\", end=\" \")\n",
    "    orig_code, orig_time = generate_code(original_model, original_tokenizer, task)\n",
    "    orig_pass = test_code(task, orig_code)\n",
    "    original_results.append({\"id\": task[\"id\"], \"passed\": orig_pass, \"time\": orig_time})\n",
    "    print(f\"{'✓ PASS' if orig_pass else '✗ FAIL'} ({orig_time:.2f}s)\")\n",
    "    \n",
    "    # Test pruned model\n",
    "    print(\"  → Pruned model...\", end=\" \")\n",
    "    pruned_code, pruned_time = generate_code(pruned_model, pruned_tokenizer, task)\n",
    "    pruned_pass = test_code(task, pruned_code)\n",
    "    pruned_results.append({\"id\": task[\"id\"], \"passed\": pruned_pass, \"time\": pruned_time})\n",
    "    print(f\"{'✓ PASS' if pruned_pass else '✗ FAIL'} ({pruned_time:.2f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "original_passed = sum(1 for r in original_results if r[\"passed\"])\n",
    "pruned_passed = sum(1 for r in pruned_results if r[\"passed\"])\n",
    "\n",
    "original_avg_time = sum(r[\"time\"] for r in original_results) / len(original_results)\n",
    "pruned_avg_time = sum(r[\"time\"] for r in pruned_results) / len(pruned_results)\n",
    "\n",
    "total_tests = len(benchmark)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"                    FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Total Problems: {total_tests}\")\n",
    "print()\n",
    "print(\"ACCURACY (How many problems solved correctly):\")\n",
    "print(f\"  Original Model: {original_passed}/{total_tests} = {original_passed/total_tests*100:.1f}%\")\n",
    "print(f\"  Pruned Model:   {pruned_passed}/{total_tests} = {pruned_passed/total_tests*100:.1f}%\")\n",
    "print(f\"  Difference:     {pruned_passed - original_passed} problems ({(pruned_passed - original_passed)/total_tests*100:+.1f}%)\")\n",
    "print()\n",
    "print(\"SPEED (Average time per problem):\")\n",
    "print(f\"  Original Model: {original_avg_time:.3f} seconds\")\n",
    "print(f\"  Pruned Model:   {pruned_avg_time:.3f} seconds\")\n",
    "print(f\"  Speedup:        {original_avg_time/pruned_avg_time:.2f}x {'faster' if pruned_avg_time < original_avg_time else 'slower'}\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show which problems each model got wrong\n",
    "original_failed = [r[\"id\"] for r in original_results if not r[\"passed\"]]\n",
    "pruned_failed = [r[\"id\"] for r in pruned_results if not r[\"passed\"]]\n",
    "\n",
    "if original_failed:\n",
    "    print(f\"\\nOriginal model failed on: {', '.join(original_failed)}\")\n",
    "if pruned_failed:\n",
    "    print(f\"Pruned model failed on: {', '.join(pruned_failed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to a JSON file\n",
    "results_data = {\n",
    "    \"summary\": {\n",
    "        \"total_tests\": total_tests,\n",
    "        \"original_accuracy\": f\"{original_passed/total_tests*100:.1f}%\",\n",
    "        \"pruned_accuracy\": f\"{pruned_passed/total_tests*100:.1f}%\",\n",
    "        \"original_avg_time\": f\"{original_avg_time:.3f}s\",\n",
    "        \"pruned_avg_time\": f\"{pruned_avg_time:.3f}s\",\n",
    "        \"speedup\": f\"{original_avg_time/pruned_avg_time:.2f}x\"\n",
    "    },\n",
    "    \"original_results\": original_results,\n",
    "    \"pruned_results\": pruned_results\n",
    "}\n",
    "\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "with open(\"../outputs/comparison_results.json\", \"w\") as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(\"Results saved to: outputs/comparison_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
