{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Original vs Pruned Model\n",
    "This notebook compares the original Qwen2.5-Coder-3B-Instruct with your pruned model.\n",
    "\n",
    "We will test both models on 20 Java coding problems and compare:\n",
    "- **Speed**: How fast each model generates code\n",
    "- **Accuracy**: How many problems each model solves correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport time\nimport json\nimport os\nimport re\nimport subprocess\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import pipeline\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    \n# Check for transformers version\nimport transformers\nprint(f\"Transformers version: {transformers.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Problems\n",
    "We'll use the first 20 problems from the Java benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 test problems\n"
     ]
    }
   ],
   "source": [
    "# Simple test problems (first 20)\n",
    "benchmark = [\n",
    "  {\"id\": \"java_001_is_prime\", \"prompt\": \"Write a Java method isPrime that returns true if n is a prime number, otherwise false.\", \"signature\": \"public static boolean isPrime(int n)\", \"tests\": [{\"input\": \"2\", \"expected\": \"true\"}, {\"input\": \"4\", \"expected\": \"false\"}, {\"input\": \"17\", \"expected\": \"true\"}]},\n",
    "  {\"id\": \"java_002_reverse_string\", \"prompt\": \"Write a Java method reverseString that returns the reversed version of the input string.\", \"signature\": \"public static String reverseString(String s)\", \"tests\": [{\"input\": \"\\\"abc\\\"\", \"expected\": \"\\\"cba\\\"\"}, {\"input\": \"\\\"hello\\\"\", \"expected\": \"\\\"olleh\\\"\"}]},\n",
    "  {\"id\": \"java_003_sum_array\", \"prompt\": \"Write a Java method sumArray that returns the sum of all elements in the given integer array.\", \"signature\": \"public static int sumArray(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"6\"}, {\"input\": \"new int[]{0, 0, 0}\", \"expected\": \"0\"}]},\n",
    "  {\"id\": \"java_004_factorial\", \"prompt\": \"Write a Java method factorial that returns n! (n factorial). Assume n is non-negative.\", \"signature\": \"public static long factorial(int n)\", \"tests\": [{\"input\": \"0\", \"expected\": \"1\"}, {\"input\": \"5\", \"expected\": \"120\"}]},\n",
    "  {\"id\": \"java_005_max_in_array\", \"prompt\": \"Write a Java method maxInArray that returns the maximum value in the given integer array.\", \"signature\": \"public static int maxInArray(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"3\"}, {\"input\": \"new int[]{-1, -5, -3}\", \"expected\": \"-1\"}]},\n",
    "  {\"id\": \"java_006_min_in_array\", \"prompt\": \"Write a Java method minInArray that returns the minimum value in the given integer array.\", \"signature\": \"public static int minInArray(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"1\"}, {\"input\": \"new int[]{-1, -5, -3}\", \"expected\": \"-5\"}]},\n",
    "  {\"id\": \"java_007_is_palindrome\", \"prompt\": \"Write a Java method isPalindrome that returns true if the given string is a palindrome.\", \"signature\": \"public static boolean isPalindrome(String s)\", \"tests\": [{\"input\": \"\\\"racecar\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"abc\\\"\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_008_count_vowels\", \"prompt\": \"Write a Java method countVowels that returns the number of vowels in the given string.\", \"signature\": \"public static int countVowels(String s)\", \"tests\": [{\"input\": \"\\\"hello\\\"\", \"expected\": \"2\"}, {\"input\": \"\\\"AEIOU\\\"\", \"expected\": \"5\"}]},\n",
    "  {\"id\": \"java_009_fibonacci\", \"prompt\": \"Write a Java method fibonacci that returns the n-th Fibonacci number.\", \"signature\": \"public static int fibonacci(int n)\", \"tests\": [{\"input\": \"0\", \"expected\": \"0\"}, {\"input\": \"5\", \"expected\": \"5\"}]},\n",
    "  {\"id\": \"java_010_find_index\", \"prompt\": \"Write a Java method findIndex that returns the index of target in the array, or -1 if not found.\", \"signature\": \"public static int findIndex(int[] arr, int target)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3, 4}, 3\", \"expected\": \"2\"}, {\"input\": \"new int[]{1, 2, 3, 4}, 5\", \"expected\": \"-1\"}]},\n",
    "  {\"id\": \"java_011_contains_duplicate\", \"prompt\": \"Write a Java method containsDuplicate that returns true if any value appears at least twice in the array.\", \"signature\": \"public static boolean containsDuplicate(int[] nums)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3, 1}\", \"expected\": \"true\"}, {\"input\": \"new int[]{1, 2, 3, 4}\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_012_max_subarray_sum\", \"prompt\": \"Write a Java method maxSubArraySum that returns the largest sum of a contiguous subarray.\", \"signature\": \"public static int maxSubArraySum(int[] nums)\", \"tests\": [{\"input\": \"new int[]{-2,1,-3,4,-1,2,1,-5,4}\", \"expected\": \"6\"}, {\"input\": \"new int[]{1}\", \"expected\": \"1\"}]},\n",
    "  {\"id\": \"java_013_two_sum\", \"prompt\": \"Write a Java method hasTwoSum that returns true if there exist two distinct indices i and j such that nums[i] + nums[j] == target.\", \"signature\": \"public static boolean hasTwoSum(int[] nums, int target)\", \"tests\": [{\"input\": \"new int[]{2, 7, 11, 15}, 9\", \"expected\": \"true\"}, {\"input\": \"new int[]{1, 2, 3}, 10\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_014_is_anagram\", \"prompt\": \"Write a Java method isAnagram that returns true if two given strings are anagrams of each other.\", \"signature\": \"public static boolean isAnagram(String s, String t)\", \"tests\": [{\"input\": \"\\\"anagram\\\", \\\"nagaram\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"rat\\\", \\\"car\\\"\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_015_remove_whitespace\", \"prompt\": \"Write a Java method removeWhitespace that returns a new string with all whitespace removed.\", \"signature\": \"public static String removeWhitespace(String s)\", \"tests\": [{\"input\": \"\\\"a b c\\\"\", \"expected\": \"\\\"abc\\\"\"}, {\"input\": \"\\\"   hello   world   \\\"\", \"expected\": \"\\\"helloworld\\\"\"}]},\n",
    "  {\"id\": \"java_016_power\", \"prompt\": \"Write a Java method power that returns x raised to the power n (x^n).\", \"signature\": \"public static long power(int x, int n)\", \"tests\": [{\"input\": \"2, 3\", \"expected\": \"8\"}, {\"input\": \"5, 2\", \"expected\": \"25\"}]},\n",
    "  {\"id\": \"java_017_is_sorted\", \"prompt\": \"Write a Java method isSortedAscending that returns true if the array is sorted in ascending order.\", \"signature\": \"public static boolean isSortedAscending(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3, 4}\", \"expected\": \"true\"}, {\"input\": \"new int[]{3, 2, 1}\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_018_second_largest\", \"prompt\": \"Write a Java method secondLargest that returns the second largest distinct number in the array.\", \"signature\": \"public static int secondLargest(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"2\"}, {\"input\": \"new int[]{5, 1, 5, 2}\", \"expected\": \"2\"}]},\n",
    "  {\"id\": \"java_019_is_rotation\", \"prompt\": \"Write a Java method isRotation that returns true if string b is a rotation of string a.\", \"signature\": \"public static boolean isRotation(String a, String b)\", \"tests\": [{\"input\": \"\\\"abcde\\\", \\\"cdeab\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"abcde\\\", \\\"abced\\\"\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_020_valid_parentheses\", \"prompt\": \"Write a Java method isValidParentheses that returns true if the input string containing brackets is valid.\", \"signature\": \"public static boolean isValidParentheses(String s)\", \"tests\": [{\"input\": \"\\\"()\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"([)]\\\"\", \"expected\": \"false\"}]}\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(benchmark)} test problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models\n",
    "\n",
    "**IMPORTANT**: Update the `PRUNED_MODEL_PATH` to point to your pruned model folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_path = r\"..\\Qwen-Coder-Prunned-New\"\n",
    "\n",
    "original_model_path = \"Qwen/Qwen2.5-Coder-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load original model\nprint(\" Loading ORIGINAL model from HuggingFace...\")\noriginal_tokenizer = AutoTokenizer.from_pretrained(original_model_path, trust_remote_code=True)\noriginal_model = AutoModelForCausalLM.from_pretrained(\n    original_model_path,\n    torch_dtype=torch.float32,  # Use float32 for CPU\n    low_cpu_mem_usage=True,      # Optimize CPU memory usage\n    trust_remote_code=True,\n    attn_implementation=\"eager\"  # Explicitly use eager attention to avoid version check\n)\nprint(f\"✓ Original model loaded on CPU\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load pruned model\nprint(\" Loading PRUNED model...\")\npruned_tokenizer = AutoTokenizer.from_pretrained(\n    pruned_model_path,\n    trust_remote_code=True,\n    local_files_only=True\n)\n\npruned_model = AutoModelForCausalLM.from_pretrained(\n    pruned_model_path,\n    torch_dtype=torch.float32,  # Use float32 for CPU\n    low_cpu_mem_usage=True,      # Optimize CPU memory usage\n    trust_remote_code=True,\n    local_files_only=True,\n    attn_implementation=\"eager\"  # Explicitly use eager attention to avoid version check\n)\n\nprint(\"✓ Pruned model loaded on CPU\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "These functions help us generate code and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions ready!\n"
     ]
    }
   ],
   "source": [
    "def generate_code(model, tokenizer, task):\n",
    "    \"\"\"\n",
    "    Generate Java code for a given task using a model.\n",
    "    Returns: (generated_code, time_taken_in_seconds)\n",
    "    \"\"\"\n",
    "    signature = task[\"signature\"]\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Write ONLY the Java method for this task:\n",
    "{task['prompt']}\n",
    "\n",
    "Signature: {signature}\n",
    "\n",
    "Write the complete method:\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate and measure time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.2,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    code = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Clean up the output\n",
    "    code = code.replace(\"```java\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    # Extract just the method\n",
    "    if \"public static\" in code:\n",
    "        code = code[code.index(\"public static\"):]\n",
    "        if \"}\" in code:\n",
    "            code = code[:code.rfind(\"}\")+1]\n",
    "    \n",
    "    return code, elapsed_time\n",
    "\n",
    "\n",
    "def test_code(task, method_code):\n",
    "    \"\"\"\n",
    "    Test if the generated code passes all test cases.\n",
    "    Returns: True if all tests pass, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse signature to get return type and method name\n",
    "        sig_match = re.search(r'public\\s+static\\s+(\\S+)\\s+([A-Za-z_][A-Za-z0-9_]*)\\s*\\(', task[\"signature\"])\n",
    "        if not sig_match:\n",
    "            return False\n",
    "        \n",
    "        return_type, method_name = sig_match.group(1), sig_match.group(2)\n",
    "        \n",
    "        # Build test code\n",
    "        test_calls = []\n",
    "        for i, test in enumerate(task[\"tests\"], 1):\n",
    "            inp, expected = test[\"input\"], test[\"expected\"]\n",
    "            \n",
    "            if return_type == \"String\":\n",
    "                condition = f\"!{method_name}({inp}).equals({expected})\"\n",
    "            else:\n",
    "                condition = f\"{method_name}({inp}) != {expected}\"\n",
    "            \n",
    "            test_calls.append(f\"if ({condition}) throw new Exception(\\\"Test {i} failed\\\");\")\n",
    "        \n",
    "        # Create full Java file\n",
    "        java_code = f\"\"\"\n",
    "public class Main {{\n",
    "    {method_code}\n",
    "    \n",
    "    public static void main(String[] args) {{\n",
    "        try {{\n",
    "            {chr(10).join('            ' + tc for tc in test_calls)}\n",
    "            System.out.println(\"OK\");\n",
    "        }} catch (Exception e) {{\n",
    "            System.out.println(\"FAIL\");\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        # Write to file\n",
    "        os.makedirs(\"temp\", exist_ok=True)\n",
    "        with open(\"temp/Main.java\", \"w\") as f:\n",
    "            f.write(java_code)\n",
    "        \n",
    "        # Compile\n",
    "        compile_result = subprocess.run([\"javac\", \"temp/Main.java\"], capture_output=True, text=True)\n",
    "        if compile_result.returncode != 0:\n",
    "            return False\n",
    "        \n",
    "        # Run\n",
    "        run_result = subprocess.run([\"java\", \"-cp\", \"temp\", \"Main\"], capture_output=True, text=True, timeout=5)\n",
    "        return run_result.stdout.strip() == \"OK\"\n",
    "        \n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Comparison\n",
    "Test both models on all problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comparison...\n",
      "\n",
      "============================================================\n",
      "\n",
      "[1/20] Testing: java_001_is_prime\n",
      "  → Original model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PASS (297.53s)\n",
      "  → Pruned model... "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pruned_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Test pruned model\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  → Pruned model...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m pruned_code, pruned_time = generate_code(\u001b[43mpruned_model\u001b[49m, pruned_tokenizer, task)\n\u001b[32m     21\u001b[39m pruned_pass = test_code(task, pruned_code)\n\u001b[32m     22\u001b[39m pruned_results.append({\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: task[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpassed\u001b[39m\u001b[33m\"\u001b[39m: pruned_pass, \u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m: pruned_time})\n",
      "\u001b[31mNameError\u001b[39m: name 'pruned_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Store results\n",
    "original_results = []\n",
    "pruned_results = []\n",
    "\n",
    "print(\"Starting comparison...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, task in enumerate(benchmark, 1):\n",
    "    print(f\"\\n[{i}/{len(benchmark)}] Testing: {task['id']}\")\n",
    "    \n",
    "    # Test original model\n",
    "    print(\"  → Original model...\", end=\" \")\n",
    "    orig_code, orig_time = generate_code(original_model, original_tokenizer, task)\n",
    "    orig_pass = test_code(task, orig_code)\n",
    "    original_results.append({\"id\": task[\"id\"], \"passed\": orig_pass, \"time\": orig_time})\n",
    "    print(f\"{'✓ PASS' if orig_pass else '✗ FAIL'} ({orig_time:.2f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "original_passed = sum(1 for r in original_results if r[\"passed\"])\n",
    "pruned_passed = sum(1 for r in pruned_results if r[\"passed\"])\n",
    "\n",
    "original_avg_time = sum(r[\"time\"] for r in original_results) / len(original_results)\n",
    "pruned_avg_time = sum(r[\"time\"] for r in pruned_results) / len(pruned_results)\n",
    "\n",
    "total_tests = len(benchmark)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"                    FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Total Problems: {total_tests}\")\n",
    "print()\n",
    "print(\"ACCURACY (How many problems solved correctly):\")\n",
    "print(f\"  Original Model: {original_passed}/{total_tests} = {original_passed/total_tests*100:.1f}%\")\n",
    "print(f\"  Pruned Model:   {pruned_passed}/{total_tests} = {pruned_passed/total_tests*100:.1f}%\")\n",
    "print(f\"  Difference:     {pruned_passed - original_passed} problems ({(pruned_passed - original_passed)/total_tests*100:+.1f}%)\")\n",
    "print()\n",
    "print(\"SPEED (Average time per problem):\")\n",
    "print(f\"  Original Model: {original_avg_time:.3f} seconds\")\n",
    "print(f\"  Pruned Model:   {pruned_avg_time:.3f} seconds\")\n",
    "print(f\"  Speedup:        {original_avg_time/pruned_avg_time:.2f}x {'faster' if pruned_avg_time < original_avg_time else 'slower'}\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show which problems each model got wrong\n",
    "original_failed = [r[\"id\"] for r in original_results if not r[\"passed\"]]\n",
    "pruned_failed = [r[\"id\"] for r in pruned_results if not r[\"passed\"]]\n",
    "\n",
    "if original_failed:\n",
    "    print(f\"\\nOriginal model failed on: {', '.join(original_failed)}\")\n",
    "if pruned_failed:\n",
    "    print(f\"Pruned model failed on: {', '.join(pruned_failed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to a JSON file\n",
    "results_data = {\n",
    "    \"summary\": {\n",
    "        \"total_tests\": total_tests,\n",
    "        \"original_accuracy\": f\"{original_passed/total_tests*100:.1f}%\",\n",
    "        \"pruned_accuracy\": f\"{pruned_passed/total_tests*100:.1f}%\",\n",
    "        \"original_avg_time\": f\"{original_avg_time:.3f}s\",\n",
    "        \"pruned_avg_time\": f\"{pruned_avg_time:.3f}s\",\n",
    "        \"speedup\": f\"{original_avg_time/pruned_avg_time:.2f}x\"\n",
    "    },\n",
    "    \"original_results\": original_results,\n",
    "    \"pruned_results\": pruned_results\n",
    "}\n",
    "\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "with open(\"../outputs/comparison_results.json\", \"w\") as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(\"Results saved to: outputs/comparison_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}