{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Original vs Pruned Model\n",
    "This notebook compares the original Qwen2.5-Coder-3B-Instruct with your pruned model.\n",
    "\n",
    "We will test both models on 20 Java coding problems and compare:\n",
    "- **Speed**: How fast each model generates code\n",
    "- **Accuracy**: How many problems each model solves correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 1060 3GB\n",
      "Transformers version: 4.57.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "# Check for transformers version\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce GTX 1060 3GB\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "print(\"CUDA version:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Problems\n",
    "We'll use the first 20 problems from the Java benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 test problems\n"
     ]
    }
   ],
   "source": [
    "# Simple test problems (first 20)\n",
    "benchmark = [\n",
    "  {\"id\": \"java_001_is_prime\", \"prompt\": \"Write a Java method isPrime that returns true if n is a prime number, otherwise false.\", \"signature\": \"public static boolean isPrime(int n)\", \"tests\": [{\"input\": \"2\", \"expected\": \"true\"}, {\"input\": \"4\", \"expected\": \"false\"}, {\"input\": \"17\", \"expected\": \"true\"}]},\n",
    "  {\"id\": \"java_002_reverse_string\", \"prompt\": \"Write a Java method reverseString that returns the reversed version of the input string.\", \"signature\": \"public static String reverseString(String s)\", \"tests\": [{\"input\": \"\\\"abc\\\"\", \"expected\": \"\\\"cba\\\"\"}, {\"input\": \"\\\"hello\\\"\", \"expected\": \"\\\"olleh\\\"\"}]},\n",
    "  {\"id\": \"java_003_sum_array\", \"prompt\": \"Write a Java method sumArray that returns the sum of all elements in the given integer array.\", \"signature\": \"public static int sumArray(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"6\"}, {\"input\": \"new int[]{0, 0, 0}\", \"expected\": \"0\"}]},\n",
    "  {\"id\": \"java_004_factorial\", \"prompt\": \"Write a Java method factorial that returns n! (n factorial). Assume n is non-negative.\", \"signature\": \"public static long factorial(int n)\", \"tests\": [{\"input\": \"0\", \"expected\": \"1\"}, {\"input\": \"5\", \"expected\": \"120\"}]},\n",
    "  {\"id\": \"java_005_max_in_array\", \"prompt\": \"Write a Java method maxInArray that returns the maximum value in the given integer array.\", \"signature\": \"public static int maxInArray(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"3\"}, {\"input\": \"new int[]{-1, -5, -3}\", \"expected\": \"-1\"}]},\n",
    "  {\"id\": \"java_006_min_in_array\", \"prompt\": \"Write a Java method minInArray that returns the minimum value in the given integer array.\", \"signature\": \"public static int minInArray(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"1\"}, {\"input\": \"new int[]{-1, -5, -3}\", \"expected\": \"-5\"}]},\n",
    "  {\"id\": \"java_007_is_palindrome\", \"prompt\": \"Write a Java method isPalindrome that returns true if the given string is a palindrome.\", \"signature\": \"public static boolean isPalindrome(String s)\", \"tests\": [{\"input\": \"\\\"racecar\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"abc\\\"\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_008_count_vowels\", \"prompt\": \"Write a Java method countVowels that returns the number of vowels in the given string.\", \"signature\": \"public static int countVowels(String s)\", \"tests\": [{\"input\": \"\\\"hello\\\"\", \"expected\": \"2\"}, {\"input\": \"\\\"AEIOU\\\"\", \"expected\": \"5\"}]},\n",
    "  {\"id\": \"java_009_fibonacci\", \"prompt\": \"Write a Java method fibonacci that returns the n-th Fibonacci number.\", \"signature\": \"public static int fibonacci(int n)\", \"tests\": [{\"input\": \"0\", \"expected\": \"0\"}, {\"input\": \"5\", \"expected\": \"5\"}]},\n",
    "  {\"id\": \"java_010_find_index\", \"prompt\": \"Write a Java method findIndex that returns the index of target in the array, or -1 if not found.\", \"signature\": \"public static int findIndex(int[] arr, int target)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3, 4}, 3\", \"expected\": \"2\"}, {\"input\": \"new int[]{1, 2, 3, 4}, 5\", \"expected\": \"-1\"}]},\n",
    "  {\"id\": \"java_011_contains_duplicate\", \"prompt\": \"Write a Java method containsDuplicate that returns true if any value appears at least twice in the array.\", \"signature\": \"public static boolean containsDuplicate(int[] nums)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3, 1}\", \"expected\": \"true\"}, {\"input\": \"new int[]{1, 2, 3, 4}\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_012_max_subarray_sum\", \"prompt\": \"Write a Java method maxSubArraySum that returns the largest sum of a contiguous subarray.\", \"signature\": \"public static int maxSubArraySum(int[] nums)\", \"tests\": [{\"input\": \"new int[]{-2,1,-3,4,-1,2,1,-5,4}\", \"expected\": \"6\"}, {\"input\": \"new int[]{1}\", \"expected\": \"1\"}]},\n",
    "  {\"id\": \"java_013_two_sum\", \"prompt\": \"Write a Java method hasTwoSum that returns true if there exist two distinct indices i and j such that nums[i] + nums[j] == target.\", \"signature\": \"public static boolean hasTwoSum(int[] nums, int target)\", \"tests\": [{\"input\": \"new int[]{2, 7, 11, 15}, 9\", \"expected\": \"true\"}, {\"input\": \"new int[]{1, 2, 3}, 10\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_014_is_anagram\", \"prompt\": \"Write a Java method isAnagram that returns true if two given strings are anagrams of each other.\", \"signature\": \"public static boolean isAnagram(String s, String t)\", \"tests\": [{\"input\": \"\\\"anagram\\\", \\\"nagaram\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"rat\\\", \\\"car\\\"\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_015_remove_whitespace\", \"prompt\": \"Write a Java method removeWhitespace that returns a new string with all whitespace removed.\", \"signature\": \"public static String removeWhitespace(String s)\", \"tests\": [{\"input\": \"\\\"a b c\\\"\", \"expected\": \"\\\"abc\\\"\"}, {\"input\": \"\\\"   hello   world   \\\"\", \"expected\": \"\\\"helloworld\\\"\"}]},\n",
    "  {\"id\": \"java_016_power\", \"prompt\": \"Write a Java method power that returns x raised to the power n (x^n).\", \"signature\": \"public static long power(int x, int n)\", \"tests\": [{\"input\": \"2, 3\", \"expected\": \"8\"}, {\"input\": \"5, 2\", \"expected\": \"25\"}]},\n",
    "  {\"id\": \"java_017_is_sorted\", \"prompt\": \"Write a Java method isSortedAscending that returns true if the array is sorted in ascending order.\", \"signature\": \"public static boolean isSortedAscending(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3, 4}\", \"expected\": \"true\"}, {\"input\": \"new int[]{3, 2, 1}\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_018_second_largest\", \"prompt\": \"Write a Java method secondLargest that returns the second largest distinct number in the array.\", \"signature\": \"public static int secondLargest(int[] arr)\", \"tests\": [{\"input\": \"new int[]{1, 2, 3}\", \"expected\": \"2\"}, {\"input\": \"new int[]{5, 1, 5, 2}\", \"expected\": \"2\"}]},\n",
    "  {\"id\": \"java_019_is_rotation\", \"prompt\": \"Write a Java method isRotation that returns true if string b is a rotation of string a.\", \"signature\": \"public static boolean isRotation(String a, String b)\", \"tests\": [{\"input\": \"\\\"abcde\\\", \\\"cdeab\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"abcde\\\", \\\"abced\\\"\", \"expected\": \"false\"}]},\n",
    "  {\"id\": \"java_020_valid_parentheses\", \"prompt\": \"Write a Java method isValidParentheses that returns true if the input string containing brackets is valid.\", \"signature\": \"public static boolean isValidParentheses(String s)\", \"tests\": [{\"input\": \"\\\"()\\\"\", \"expected\": \"true\"}, {\"input\": \"\\\"([)]\\\"\", \"expected\": \"false\"}]}\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(benchmark)} test problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models\n",
    "\n",
    "**IMPORTANT**: Update the `PRUNED_MODEL_PATH` to point to your pruned model folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_50_path = \"C:/Users/namnd/Documents/QwenCoder-50\"\n",
    "\n",
    "pruned_model_5_path = \"C:/Users/namnd/Documents/QwenCoder-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading PRUNED model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 54.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pruned model loaded on GPU\n",
      "\n",
      "5% model device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pruned model (5%)\n",
    "print(\" Loading PRUNED model...\")\n",
    "pruned_tokenizer_5 = AutoTokenizer.from_pretrained(\n",
    "    pruned_model_5_path,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "pruned_model_5 = AutoModelForCausalLM.from_pretrained(\n",
    "    pruned_model_5_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,      # Optimize CPU memory usage\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "print(f\"✓ Pruned model loaded on GPU\\n\")\n",
    "print(\"5% model device:\", next(pruned_model_5.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading PRUNED model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.75it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pruned model loaded on GPU\n",
      "\n",
      "50% model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load pruned model\n",
    "\n",
    "print(\" Loading PRUNED model...\")\n",
    "pruned_tokenizer_50 = AutoTokenizer.from_pretrained(\n",
    "    pruned_model_50_path,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pruned_model_50 = AutoModelForCausalLM.from_pretrained(\n",
    "    pruned_model_50_path,\n",
    "    torch_dtype=torch.float16,  # Use float16 for CPU\n",
    "    low_cpu_mem_usage=True,      # Optimize CPU memory usage\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"✓ Pruned model loaded on GPU\\n\")\n",
    "print(\"50% model device:\", next(pruned_model_50.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "These functions help us generate code and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions ready!\n"
     ]
    }
   ],
   "source": [
    "def generate_code(model, tokenizer, task):\n",
    "    \"\"\"\n",
    "    Generate Java code for a given task using a model.\n",
    "    Returns: (generated_code, time_taken_in_seconds)\n",
    "    \"\"\"\n",
    "    signature = task[\"signature\"]\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a strict Java code generator.\n",
    "  Write ONLY the Java method, this task\n",
    "You MUST follow these unbreakable rules:\n",
    "\n",
    "1. Output EXACTLY ONE Java method.\n",
    "2. ZERO explanations.\n",
    "3. ZERO repeated methods.\n",
    "4. ZERO comments.\n",
    "5. ZERO blank copies of the method.\n",
    "6. Output MUST start with 'public static'.\n",
    "7. Output MUST end with the closing bracket }} of that method.\n",
    "Write ONLY the Java method, NO EXPLANATION, ONLY WRITE THE METHOD ONE TIME for this task:\n",
    "{task['prompt']}\n",
    "\n",
    "Signature: {signature}\n",
    "\n",
    "Write the complete method:\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate and measure time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.2,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Decode only the new tokens\n",
    "    gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    code = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Clean up the output\n",
    "    code = code.replace(\"```java\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    # Extract just the method\n",
    "    if \"public static\" in code:\n",
    "        code = code[code.index(\"public static\"):]\n",
    "        if \"}\" in code:\n",
    "            code = code[:code.rfind(\"}\")+1]\n",
    "\n",
    "    return code, elapsed_time\n",
    "\n",
    "\n",
    "def test_code(task, method_code):\n",
    "    \"\"\"\n",
    "    Test if the generated code passes all test cases.\n",
    "    Returns: True if all tests pass, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse signature to get return type and method name\n",
    "        sig_match = re.search(r'public\\s+static\\s+(\\S+)\\s+([A-Za-z_][A-Za-z0-9_]*)\\s*\\(', task[\"signature\"])\n",
    "        if not sig_match:\n",
    "            return False\n",
    "\n",
    "        return_type, method_name = sig_match.group(1), sig_match.group(2)\n",
    "\n",
    "        # Build test code\n",
    "        test_calls = []\n",
    "        for i, test in enumerate(task[\"tests\"], 1):\n",
    "            inp, expected = test[\"input\"], test[\"expected\"]\n",
    "\n",
    "            if return_type == \"String\":\n",
    "                condition = f\"!{method_name}({inp}).equals({expected})\"\n",
    "            else:\n",
    "                condition = f\"{method_name}({inp}) != {expected}\"\n",
    "\n",
    "            test_calls.append(f\"if ({condition}) throw new Exception(\\\"Test {i} failed\\\");\")\n",
    "\n",
    "        # Create full Java file\n",
    "        java_code = f\"\"\"\n",
    "public class Main {{\n",
    "    {method_code}\n",
    "\n",
    "    public static void main(String[] args) {{\n",
    "        try {{\n",
    "            {chr(10).join('            ' + tc for tc in test_calls)}\n",
    "            System.out.println(\"OK\");\n",
    "        }} catch (Exception e) {{\n",
    "            System.out.println(\"FAIL\");\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        # Write to file\n",
    "        os.makedirs(\"temp\", exist_ok=True)\n",
    "        with open(\"temp/Main.java\", \"w\") as f:\n",
    "            f.write(java_code)\n",
    "\n",
    "        # Compile\n",
    "        compile_result = subprocess.run([\"javac\", \"temp/Main.java\"], capture_output=True, text=True)\n",
    "        if compile_result.returncode != 0:\n",
    "            return False\n",
    "\n",
    "        # Run\n",
    "        run_result = subprocess.run([\"java\", \"-cp\", \"temp\", \"Main\"], capture_output=True, text=True, timeout=5)\n",
    "        return run_result.stdout.strip() == \"OK\"\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: C:/Users/namnd/Documents/Pruning-LLM/output\n"
     ]
    }
   ],
   "source": [
    "output_path = \"C:/Users/namnd/Documents/Pruning-LLM/output\"\n",
    "if os.path.exists(output_path):\n",
    "   print(\"File found:\", output_path)\n",
    "else:\n",
    "   print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Comparison\n",
    "Test both models on all problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comparison...\n",
      "\n",
      "============================================================\n",
      "\n",
      "[1/20] Testing: java_001_is_prime\n",
      "  → 50% Pruned model... ✗ FAIL (192.81s)\n",
      "\n",
      "[2/20] Testing: java_002_reverse_string\n",
      "  → 50% Pruned model... ✗ FAIL (73.29s)\n",
      "\n",
      "[3/20] Testing: java_003_sum_array\n",
      "  → 50% Pruned model... ✗ FAIL (84.24s)\n",
      "\n",
      "[4/20] Testing: java_004_factorial\n",
      "  → 50% Pruned model... ✗ FAIL (305.16s)\n",
      "\n",
      "[5/20] Testing: java_005_max_in_array\n",
      "  → 50% Pruned model... ✗ FAIL (160.14s)\n",
      "\n",
      "[6/20] Testing: java_006_min_in_array\n",
      "  → 50% Pruned model... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 50% PRUNED MODEL\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  → 50\u001b[39m\u001b[33m%\u001b[39m\u001b[33m Pruned model...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m pruned_50_code, pruned_50_time = \u001b[43mgenerate_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruned_model_50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpruned_tokenizer_50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m pruned_50_pass = test_code(task, pruned_50_code)\n\u001b[32m     20\u001b[39m pruned_50_results.append({\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: task_id, \u001b[33m\"\u001b[39m\u001b[33mpassed\u001b[39m\u001b[33m\"\u001b[39m: pruned_50_pass, \u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m: pruned_50_time\n\u001b[32m     22\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mgenerate_code\u001b[39m\u001b[34m(model, tokenizer, task)\u001b[39m\n\u001b[32m     32\u001b[39m start_time = time.time()\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     output_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m elapsed_time = time.time() - start_time\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Decode only the new tokens\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:449\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    430\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    431\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    432\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    434\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\utils\\generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:384\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    397\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    398\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    399\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:249\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m residual = hidden_states\n\u001b[32m    248\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:46\u001b[39m, in \u001b[36mQwen2MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     down_proj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\accelerate\\hooks.py:170\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\accelerate\\hooks.py:360\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    353\u001b[39m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    354\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    355\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m value.data_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map\n\u001b[32m    356\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map[value.data_ptr()]\n\u001b[32m    357\u001b[39m         ):\n\u001b[32m    358\u001b[39m             \u001b[38;5;28mself\u001b[39m.tied_pointers_to_remove.add((value.data_ptr(), \u001b[38;5;28mself\u001b[39m.execution_device))\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m.execution_device), send_to_device(\n\u001b[32m    370\u001b[39m     kwargs, \u001b[38;5;28mself\u001b[39m.execution_device, skip_keys=\u001b[38;5;28mself\u001b[39m.skip_keys\n\u001b[32m    371\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnd\\AppData\\Local\\miniconda3\\envs\\prune_llm\\Lib\\site-packages\\accelerate\\utils\\modeling.py:343\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[39m\n\u001b[32m    341\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     new_value = torch.tensor(value, device=device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "output_file = open(os.path.join(output_path, \"model_comparison_results.txt\"), \"w\")\n",
    "# Store accuracy & speed results\n",
    "pruned_5_results = []\n",
    "pruned_50_results = []\n",
    "\n",
    "\n",
    "print(\"Starting comparison...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, task in enumerate(benchmark, 1):\n",
    "    task_id = task[\"id\"]\n",
    "    print(f\"\\n[{i}/{len(benchmark)}] Testing: {task_id}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 50% PRUNED MODEL\n",
    "    # ==========================================\n",
    "    print(\"  → 50% Pruned model...\", end=\" \")\n",
    "    pruned_50_code, pruned_50_time = generate_code(pruned_model_50, pruned_tokenizer_50, task)\n",
    "    pruned_50_pass = test_code(task, pruned_50_code)\n",
    "    pruned_50_results.append({\n",
    "        \"id\": task_id, \"passed\": pruned_50_pass, \"time\": pruned_50_time\n",
    "    })\n",
    "    print(f\"{'✓ PASS' if pruned_50_pass else '✗ FAIL'} ({pruned_50_time:.2f}s)\")\n",
    "\n",
    "    # ==========================================\n",
    "    # WRITE RESULTS TO SINGLE OUTPUT FILE\n",
    "    # ==========================================\n",
    "    output_file.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    output_file.write(f\"TASK: {task_id}\\n\")\n",
    "    output_file.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    output_file.write(\"--- 50% PRUNED MODEL OUTPUT ---\\n\")\n",
    "    output_file.write(pruned_50_code + \"\\n\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison complete!\")\n",
    "print(f\"Saved everything to: {os.path.join(output_path, 'model_comparison_results.txt')}\")\n",
    "\n",
    "output_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # ==========================================\n",
    "    # 5% PRUNED MODEL\n",
    "    # ==========================================\n",
    "    print(\"  → 5% Pruned model...\", end=\" \")\n",
    "    pruned_5_code, pruned_5_time = generate_code(pruned_model_5, pruned_tokenizer_5, task)\n",
    "    pruned_5_pass = test_code(task, pruned_5_code)\n",
    "    pruned_5_results.append({\n",
    "        \"id\": task_id, \"passed\": pruned_5_pass, \"time\": pruned_5_time\n",
    "    })\n",
    "    print(f\"{'✓ PASS' if pruned_5_pass else '✗ FAIL'} ({pruned_5_time:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "original_passed = sum(1 for r in original_results if r[\"passed\"])\n",
    "pruned_passed = sum(1 for r in pruned_results if r[\"passed\"])\n",
    "\n",
    "original_avg_time = sum(r[\"time\"] for r in original_results) / len(original_results)\n",
    "pruned_avg_time = sum(r[\"time\"] for r in pruned_results) / len(pruned_results)\n",
    "\n",
    "total_tests = len(benchmark)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"                    FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Total Problems: {total_tests}\")\n",
    "print()\n",
    "print(\"ACCURACY (How many problems solved correctly):\")\n",
    "print(f\"  Original Model: {original_passed}/{total_tests} = {original_passed/total_tests*100:.1f}%\")\n",
    "print(f\"  Pruned Model:   {pruned_passed}/{total_tests} = {pruned_passed/total_tests*100:.1f}%\")\n",
    "print(f\"  Difference:     {pruned_passed - original_passed} problems ({(pruned_passed - original_passed)/total_tests*100:+.1f}%)\")\n",
    "print()\n",
    "print(\"SPEED (Average time per problem):\")\n",
    "print(f\"  Original Model: {original_avg_time:.3f} seconds\")\n",
    "print(f\"  Pruned Model:   {pruned_avg_time:.3f} seconds\")\n",
    "print(f\"  Speedup:        {original_avg_time/pruned_avg_time:.2f}x {'faster' if pruned_avg_time < original_avg_time else 'slower'}\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show which problems each model got wrong\n",
    "original_failed = [r[\"id\"] for r in original_results if not r[\"passed\"]]\n",
    "pruned_failed = [r[\"id\"] for r in pruned_results if not r[\"passed\"]]\n",
    "\n",
    "if original_failed:\n",
    "    print(f\"\\nOriginal model failed on: {', '.join(original_failed)}\")\n",
    "if pruned_failed:\n",
    "    print(f\"Pruned model failed on: {', '.join(pruned_failed)}\")\n",
    "\n",
    "# Save generated code to files\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "\n",
    "# Check if generated code was captured\n",
    "if original_results and \"generated_code\" in original_results[0]:\n",
    "    # Save original model solutions\n",
    "    with open(\"../outputs/original_model_solutions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"ORIGINAL MODEL GENERATED SOLUTIONS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        for result in original_results:\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
    "            f.write(f\"Problem: {result['id']}\\n\")\n",
    "            f.write(f\"Status: {'PASS ✓' if result['passed'] else 'FAIL ✗'}\\n\")\n",
    "            f.write(f\"Time: {result['time']:.2f}s\\n\")\n",
    "            f.write(f\"{'='*80}\\n\\n\")\n",
    "            f.write(result['generated_code'])\n",
    "            f.write(\"\\n\\n\")\n",
    "    print(\"\\n✓ Original model solutions saved to: outputs/original_model_solutions.txt\")\n",
    "    \n",
    "    # Save pruned model solutions\n",
    "    with open(\"../outputs/pruned_model_solutions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"PRUNED MODEL GENERATED SOLUTIONS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        for result in pruned_results:\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
    "            f.write(f\"Problem: {result['id']}\\n\")\n",
    "            f.write(f\"Status: {'PASS ✓' if result['passed'] else 'FAIL ✗'}\\n\")\n",
    "            f.write(f\"Time: {result['time']:.2f}s\\n\")\n",
    "            f.write(f\"{'='*80}\\n\\n\")\n",
    "            f.write(result['generated_code'])\n",
    "            f.write(\"\\n\\n\")\n",
    "    print(\"✓ Pruned model solutions saved to: outputs/pruned_model_solutions.txt\")\n",
    "else:\n",
    "    print(\"\\n⚠ Generated code not available in current results.\")\n",
    "    print(\"To capture generated code, modify cell 12 to add 'generated_code' to results:\")\n",
    "    print(\"  original_results.append({'id': ..., 'passed': ..., 'time': ..., 'generated_code': orig_code})\")\n",
    "    print(\"  pruned_results.append({'id': ..., 'passed': ..., 'time': ..., 'generated_code': pruned_code})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Save Generated Code\n",
    "Re-generate and save the code solutions from both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Prompting\n",
    "Test both models with custom prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
