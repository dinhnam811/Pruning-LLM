{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Instruction Following Evaluation\n",
    "\n",
    "This notebook evaluates **multiple LLM models** simultaneously on instruction following tasks.\n",
    "\n",
    "**Features:**\n",
    "- Load 2-3 models at once\n",
    "- Compare ROUGE, BLEU, and Semantic Similarity\n",
    "- Side-by-side response comparison\n",
    "- Visualization charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install rouge-score nltk sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data if needed\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load JSONL dataset.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load the instruction following dataset\n",
    "dataset_path = \"../data/eval/instruction_samples_full.jsonl\"  # Update path if needed\n",
    "dataset = load_dataset(dataset_path)\n",
    "print(f\"Loaded {len(dataset)} instruction samples\")\n",
    "print(f\"\\nExample sample:\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configuration\n",
    "\n",
    "**Edit this section to add/remove models you want to compare**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all models you want to evaluate\n",
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"Original\",\n",
    "        \"path\": \"Qwen/Qwen2.5-Coder-3B-Instruct\",\n",
    "        \"color\": \"skyblue\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Pruned-5%\",\n",
    "        \"path\": \"path/to/your/pruned-5-model\",  # Update this path\n",
    "        \"color\": \"lightcoral\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Pruned-20%\",\n",
    "        \"path\": \"path/to/your/pruned-20-model\",  # Update this path\n",
    "        \"color\": \"lightgreen\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Will evaluate {len(MODEL_CONFIGS)} models:\")\n",
    "for config in MODEL_CONFIGS:\n",
    "    print(f\"  - {config['name']}: {config['path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(configs: List[Dict]) -> Dict[str, Dict]:\n",
    "    \"\"\"Load multiple models and tokenizers.\n",
    "\n",
    "    Args:\n",
    "        configs: List of model configurations\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping model names to {tokenizer, model} dicts\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "\n",
    "    for config in configs:\n",
    "        name = config[\"name\"]\n",
    "        path = config[\"path\"]\n",
    "\n",
    "        print(f\"\\nLoading {name} from {path}...\")\n",
    "\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "\n",
    "            models[name] = {\n",
    "                \"tokenizer\": tokenizer,\n",
    "                \"model\": model,\n",
    "                \"config\": config\n",
    "            }\n",
    "\n",
    "            print(f\"✓ {name} loaded successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to load {name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING ALL MODELS\")\n",
    "print(\"=\"*60)\n",
    "models = load_models(MODEL_CONFIGS)\n",
    "print(f\"\\n✓ Successfully loaded {len(models)} models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_prompt(sample: Dict[str, Any]) -> str:\n    \"\"\"Create a prompt for instruction following.\"\"\"\n    prompt = sample['prompt']\n    formatted_prompt = f\"\"\"Question: {prompt}\n\nAnswer:\"\"\"\n    return formatted_prompt\n\ndef generate_response(prompt: str, tokenizer, model, max_length: int = 256, use_fast_mode: bool = True) -> str:\n    \"\"\"Generate response from a specific model.\n\n    Args:\n        prompt: The input prompt\n        tokenizer: Model's tokenizer\n        model: Model to generate with\n        max_length: Maximum length of generated tokens\n        use_fast_mode: If True, use greedy decoding (faster). If False, use sampling (more diverse but slower)\n\n    Returns:\n        Generated response string\n    \"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # Generate response\n    with torch.no_grad():\n        if use_fast_mode:\n            # FAST MODE: Greedy decoding (deterministic, much faster)\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_length,\n                do_sample=False,              # Greedy decoding (no sampling)\n                temperature=0.2,\n                repetition_penalty=1.2,\n                pad_token_id=tokenizer.eos_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        else:\n            # SLOW MODE: Sampling (diverse but slower)\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_length,\n                do_sample=True,               # Sampling (random)\n                temperature=0.7,\n                top_p=0.9,\n                pad_token_id=tokenizer.eos_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n\n    # Decode output (same as your fast method)\n    gen_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n    response = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n\n    return response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Generation on All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation on all models\n",
    "print(\"=\"*60)\n",
    "print(\"TEST GENERATION ON ALL MODELS\")\n",
    "print(\"=\"*60)\n",
    "test_sample = dataset[0]\n",
    "test_prompt = create_prompt(test_sample)\n",
    "print(f\"Prompt: {test_sample['prompt']}\")\n",
    "print(f\"Expected: {test_sample['expected_response']}\\n\")\n",
    "\n",
    "for model_name, model_data in models.items():\n",
    "    print(f\"[{model_name}] Generating...\")\n",
    "    response = generate_response(\n",
    "        test_prompt,\n",
    "        model_data[\"tokenizer\"],\n",
    "        model_data[\"model\"]\n",
    "    )\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(generated: str, reference: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate ROUGE scores.\n",
    "    \n",
    "    ROUGE-1: Unigram overlap\n",
    "    ROUGE-2: Bigram overlap\n",
    "    ROUGE-L: Longest common subsequence\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "\n",
    "    return {\n",
    "        'rouge1_f': scores['rouge1'].fmeasure,\n",
    "        'rouge2_f': scores['rouge2'].fmeasure,\n",
    "        'rougeL_f': scores['rougeL'].fmeasure,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model for semantic similarity\n",
    "print(\"Loading sentence embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded!\")\n",
    "\n",
    "def calculate_semantic_similarity(generated: str, reference: str) -> float:\n",
    "    \"\"\"Calculate semantic similarity using sentence embeddings.\n",
    "    \n",
    "    Returns cosine similarity between embeddings (0 to 1).\n",
    "    \"\"\"\n",
    "    embeddings = embedding_model.encode([generated, reference])\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return float(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Model Evaluation\n",
    "\n",
    "This will evaluate all loaded models on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models(models: Dict, dataset: List[Dict]) -> Dict[str, Dict]:\n",
    "    \"\"\"Evaluate all models on the dataset.\n",
    "\n",
    "    Args:\n",
    "        models: Dictionary of model_name -> {tokenizer, model, config}\n",
    "        dataset: List of instruction samples\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping model names to their evaluation results\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    for model_name, model_data in models.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATING: {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        tokenizer = model_data[\"tokenizer\"]\n",
    "        model = model_data[\"model\"]\n",
    "\n",
    "        results = {\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'semantic_similarity': [],\n",
    "            'samples': []\n",
    "        }\n",
    "\n",
    "        for idx, sample in enumerate(tqdm(dataset, desc=f\"{model_name}\")):\n",
    "            # Create prompt and generate response\n",
    "            prompt = create_prompt(sample)\n",
    "            generated = generate_response(prompt, tokenizer, model)\n",
    "            reference = sample['expected_response']\n",
    "\n",
    "            # Calculate all metrics\n",
    "            rouge_scores = calculate_rouge(generated, reference)\n",
    "            sem_sim = calculate_semantic_similarity(generated, reference)\n",
    "\n",
    "            # Store metrics\n",
    "            results['rouge1'].append(rouge_scores['rouge1_f'])\n",
    "            results['rouge2'].append(rouge_scores['rouge2_f'])\n",
    "            results['rougeL'].append(rouge_scores['rougeL_f'])\n",
    "            results['semantic_similarity'].append(sem_sim)\n",
    "\n",
    "            # Store sample results\n",
    "            results['samples'].append({\n",
    "                'idx': idx,\n",
    "                'id': sample['id'],\n",
    "                'prompt': sample['prompt'],\n",
    "                'generated': generated,\n",
    "                'reference': reference,\n",
    "                'rouge1': rouge_scores['rouge1_f'],\n",
    "                'rouge2': rouge_scores['rouge2_f'],\n",
    "                'rougeL': rouge_scores['rougeL_f'],\n",
    "                'semantic_similarity': sem_sim\n",
    "            })\n",
    "\n",
    "        # Calculate aggregate metrics\n",
    "        results['aggregate'] = {\n",
    "            'rouge1': np.mean(results['rouge1']),\n",
    "            'rouge2': np.mean(results['rouge2']),\n",
    "            'rougeL': np.mean(results['rougeL']),\n",
    "            'semantic_similarity': np.mean(results['semantic_similarity'])\n",
    "        }\n",
    "\n",
    "        all_results[model_name] = results\n",
    "\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"  ROUGE-1: {results['aggregate']['rouge1']:.4f}\")\n",
    "        print(f\"  ROUGE-2: {results['aggregate']['rouge2']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {results['aggregate']['rougeL']:.4f}\")\n",
    "        print(f\"  Semantic Similarity: {results['aggregate']['semantic_similarity']:.4f}\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on all models\n",
    "# For quick testing, use dataset[:5] instead of full dataset\n",
    "all_eval_results = evaluate_all_models(models, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON OF ALL MODELS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset size: {len(dataset)} samples\\n\")\n",
    "\n",
    "# Create comparison table\n",
    "print(f\"{'Model':<20} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10} {'Semantic':<10}\")\n",
    "print(\"-\"*60)\n",
    "for model_name, results in all_eval_results.items():\n",
    "    agg = results['aggregate']\n",
    "    print(f\"{model_name:<20} {agg['rouge1']:<10.4f} {agg['rouge2']:<10.4f} {agg['rougeL']:<10.4f} {agg['semantic_similarity']:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. ROUGE Scores Comparison\n",
    "ax1 = axes[0]\n",
    "model_names = list(all_eval_results.keys())\n",
    "rouge1_scores = [all_eval_results[m]['aggregate']['rouge1'] for m in model_names]\n",
    "rouge2_scores = [all_eval_results[m]['aggregate']['rouge2'] for m in model_names]\n",
    "rougeL_scores = [all_eval_results[m]['aggregate']['rougeL'] for m in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x - width, rouge1_scores, width, label='ROUGE-1', color='skyblue')\n",
    "ax1.bar(x, rouge2_scores, width, label='ROUGE-2', color='lightcoral')\n",
    "ax1.bar(x + width, rougeL_scores, width, label='ROUGE-L', color='lightgreen')\n",
    "\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('ROUGE Scores Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# 2. Semantic Similarity Comparison\n",
    "ax2 = axes[1]\n",
    "sem_sim_scores = [all_eval_results[m]['aggregate']['semantic_similarity'] for m in model_names]\n",
    "colors = [models[m]['config']['color'] for m in model_names]\n",
    "\n",
    "bars = ax2.bar(model_names, sem_sim_scores, color=colors)\n",
    "ax2.set_xlabel('Models')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Semantic Similarity Comparison')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, score) in enumerate(zip(bars, sem_sim_scores)):\n",
    "    ax2.text(i, score + 0.02, f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison of first few samples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIDE-BY-SIDE COMPARISON (First 3 Samples)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(3, len(dataset))):\n",
    "    sample = dataset[i]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sample {i+1}: {sample['prompt']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Expected: {sample['expected_response']}\\n\")\n",
    "\n",
    "    for model_name, results in all_eval_results.items():\n",
    "        sample_result = results['samples'][i]\n",
    "        print(f\"[{model_name}]\")\n",
    "        print(f\"  Generated: {sample_result['generated']}\")\n",
    "        print(f\"  ROUGE-L: {sample_result['rougeL']:.3f} | Semantic Sim: {sample_result['semantic_similarity']:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "output_path = \"../data/eval/multi_model_comparison_results.json\"\n",
    "\n",
    "json_results = {\n",
    "    'dataset_size': len(dataset),\n",
    "    'models': list(all_eval_results.keys()),\n",
    "    'aggregate_comparison': {\n",
    "        model_name: results['aggregate']\n",
    "        for model_name, results in all_eval_results.items()\n",
    "    },\n",
    "    'detailed_results': {\n",
    "        model_name: {\n",
    "            'aggregate': results['aggregate'],\n",
    "            'first_5_samples': results['samples'][:5]\n",
    "        }\n",
    "        for model_name, results in all_eval_results.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Best and Worst Performers\n",
    "\n",
    "Find which model performs best/worst on each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze best/worst performing model for each sample\n",
    "model_names = list(all_eval_results.keys())\n",
    "best_model_count = {name: 0 for name in model_names}\n",
    "worst_model_count = {name: 0 for name in model_names}\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    # Get semantic similarity scores for this sample from all models\n",
    "    scores = {name: all_eval_results[name]['samples'][i]['semantic_similarity'] \n",
    "              for name in model_names}\n",
    "    \n",
    "    best_model = max(scores, key=scores.get)\n",
    "    worst_model = min(scores, key=scores.get)\n",
    "    \n",
    "    best_model_count[best_model] += 1\n",
    "    worst_model_count[worst_model] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST PERFORMER BY SAMPLE COUNT\")\n",
    "print(\"=\"*60)\n",
    "for name, count in sorted(best_model_count.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name:<20}: Best on {count}/{len(dataset)} samples ({count/len(dataset)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORST PERFORMER BY SAMPLE COUNT\")\n",
    "print(\"=\"*60)\n",
    "for name, count in sorted(worst_model_count.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name:<20}: Worst on {count}/{len(dataset)} samples ({count/len(dataset)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}