{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraint-Based Instruction Following Evaluation\n",
    "\n",
    "Simple pipeline: Load → Generate → Test → Eval Full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Hugging Face\n",
    "ds = load_dataset(\"wis-k/instruction-following-eval\")\n",
    "data = ds[\"train\"].select(range(30))  # Take first 30 samples\n",
    "\n",
    "print(f\"Loaded {len(data)} samples\")\n",
    "print(\"\\nFirst sample:\")\n",
    "print(f\"Prompt: {data[0]['prompt'][:100]}...\")\n",
    "print(f\"Constraints: {data[0]['instruction_id_list']}\")\n",
    "print(f\"Kwargs: {data[0]['kwargs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_PATH}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"✓ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Test Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_tokens=1024):\n",
    "    \"\"\"Generate response from model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_text[len(prompt):].strip() if prompt in full_text else full_text.strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Generate test response\n",
    "test_prompt = data[0]['prompt']\n",
    "test_response = generate_response(test_prompt)\n",
    "\n",
    "print(\"Test Response:\")\n",
    "print(\"=\" * 80)\n",
    "print(test_response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Constraint Checking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_constraint(response, constraint_type, kwargs):\n",
    "    \"\"\"Check if response satisfies a constraint.\"\"\"\n",
    "    \n",
    "    # Punctuation: no comma\n",
    "    if constraint_type == \"punctuation:no_comma\":\n",
    "        passed = ',' not in response\n",
    "        return passed, f\"Commas found: {response.count(',')}\"\n",
    "    \n",
    "    # Format: highlighted sections\n",
    "    elif constraint_type == \"detectable_format:number_highlighted_sections\":\n",
    "        num_required = kwargs.get('num_highlights', 0)\n",
    "        highlights = len(re.findall(r'\\*[^*]+\\*', response))\n",
    "        passed = highlights >= num_required\n",
    "        return passed, f\"Highlighted: {highlights} (need {num_required}+)\"\n",
    "    \n",
    "    # Length: word count\n",
    "    elif constraint_type == \"length_constraints:number_words\":\n",
    "        words = len(response.split())\n",
    "        required = kwargs.get('num_words', 0)\n",
    "        relation = kwargs.get('relation', 'at least')\n",
    "        \n",
    "        if relation == 'at least':\n",
    "            passed = words >= required\n",
    "        elif relation == 'at most':\n",
    "            passed = words <= required\n",
    "        else:\n",
    "            passed = words == required\n",
    "        \n",
    "        return passed, f\"Words: {words} ({relation} {required})\"\n",
    "    \n",
    "    # Length: sentence count\n",
    "    elif constraint_type == \"length_constraints:number_sentences\":\n",
    "        sentences = len(re.findall(r'[.!?]+', response))\n",
    "        required = kwargs.get('num_sentences', 0)\n",
    "        relation = kwargs.get('relation', 'exactly')\n",
    "        \n",
    "        if relation == 'at least':\n",
    "            passed = sentences >= required\n",
    "        elif relation == 'at most':\n",
    "            passed = sentences <= required\n",
    "        else:\n",
    "            passed = sentences == required\n",
    "        \n",
    "        return passed, f\"Sentences: {sentences} ({relation} {required})\"\n",
    "    \n",
    "    # Keywords: existence\n",
    "    elif constraint_type == \"keywords:existence\":\n",
    "        keywords = kwargs.get('keywords', [])\n",
    "        response_lower = response.lower()\n",
    "        missing = [k for k in keywords if k.lower() not in response_lower]\n",
    "        passed = len(missing) == 0\n",
    "        return passed, f\"Missing: {missing}\" if missing else \"All keywords present\"\n",
    "    \n",
    "    # End checker\n",
    "    elif constraint_type == \"startend:end_checker\":\n",
    "        end_phrase = kwargs.get('end_phrase', '')\n",
    "        passed = response.strip().endswith(end_phrase)\n",
    "        return passed, f\"Ends with '{end_phrase}': {passed}\"\n",
    "    \n",
    "    # Default: unknown constraint\n",
    "    else:\n",
    "        return False, f\"Unknown: {constraint_type}\"\n",
    "\n",
    "print(\"✓ Constraint checking functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Test Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single(response, instruction_id_list, kwargs):\n",
    "    \"\"\"Evaluate a single response against constraints.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for constraint_type in instruction_id_list:\n",
    "        passed, explanation = check_constraint(response, constraint_type, kwargs)\n",
    "        results.append({\n",
    "            'constraint': constraint_type,\n",
    "            'passed': passed,\n",
    "            'explanation': explanation\n",
    "        })\n",
    "    \n",
    "    overall_pass = all(r['passed'] for r in results)\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'overall_pass': overall_pass,\n",
    "        'passed': sum(r['passed'] for r in results),\n",
    "        'total': len(results)\n",
    "    }\n",
    "\n",
    "# Evaluate test response\n",
    "test_eval = evaluate_single(\n",
    "    test_response,\n",
    "    data[0]['instruction_id_list'],\n",
    "    data[0]['kwargs']\n",
    ")\n",
    "\n",
    "print(\"Test Evaluation Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Overall: {'PASS ✓' if test_eval['overall_pass'] else 'FAIL ✗'}\")\n",
    "print(f\"Score: {test_eval['passed']}/{test_eval['total']}\\n\")\n",
    "\n",
    "for r in test_eval['results']:\n",
    "    status = '✓' if r['passed'] else '✗'\n",
    "    print(f\"{status} {r['constraint']}\")\n",
    "    print(f\"  → {r['explanation']}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all samples\n",
    "all_results = []\n",
    "constraint_stats = {}\n",
    "\n",
    "for idx in tqdm(range(len(data)), desc=\"Evaluating\"):\n",
    "    sample = data[idx]\n",
    "    \n",
    "    # Generate\n",
    "    response = generate_response(sample['prompt'])\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_result = evaluate_single(\n",
    "        response,\n",
    "        sample['instruction_id_list'],\n",
    "        sample['kwargs']\n",
    "    )\n",
    "    \n",
    "    # Store\n",
    "    all_results.append({\n",
    "        'idx': idx,\n",
    "        'prompt': sample['prompt'],\n",
    "        'response': response,\n",
    "        'eval': eval_result\n",
    "    })\n",
    "    \n",
    "    # Track per-constraint stats\n",
    "    for r in eval_result['results']:\n",
    "        constraint = r['constraint']\n",
    "        if constraint not in constraint_stats:\n",
    "            constraint_stats[constraint] = []\n",
    "        constraint_stats[constraint].append(r['passed'])\n",
    "\n",
    "print(\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "total_samples = len(all_results)\n",
    "passed_samples = sum(r['eval']['overall_pass'] for r in all_results)\n",
    "overall_pass_rate = passed_samples / total_samples\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  Samples: {total_samples}\")\n",
    "print(f\"  Passed: {passed_samples}\")\n",
    "print(f\"  Failed: {total_samples - passed_samples}\")\n",
    "print(f\"  Pass Rate: {overall_pass_rate:.1%}\")\n",
    "\n",
    "print(f\"\\nPer-Constraint Accuracy:\")\n",
    "for constraint, passes in sorted(constraint_stats.items()):\n",
    "    accuracy = sum(passes) / len(passes)\n",
    "    print(f\"  {constraint}:\")\n",
    "    print(f\"    {accuracy:.1%} ({sum(passes)}/{len(passes)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample Results (First 3):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for result in all_results[:3]:\n",
    "    print(f\"\\n[Sample {result['idx']}]\")\n",
    "    print(f\"Prompt: {result['prompt'][:80]}...\")\n",
    "    print(f\"Overall: {'PASS ✓' if result['eval']['overall_pass'] else 'FAIL ✗'}\")\n",
    "    print(f\"Score: {result['eval']['passed']}/{result['eval']['total']}\")\n",
    "    \n",
    "    for r in result['eval']['results']:\n",
    "        status = '✓' if r['passed'] else '✗'\n",
    "        print(f\"  {status} {r['constraint']}: {r['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "output = {\n",
    "    'model': MODEL_PATH,\n",
    "    'total_samples': total_samples,\n",
    "    'overall_pass_rate': overall_pass_rate,\n",
    "    'constraint_accuracy': {k: sum(v)/len(v) for k, v in constraint_stats.items()},\n",
    "    'all_results': all_results\n",
    "}\n",
    "\n",
    "with open('eval_results.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"✓ Results saved to eval_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
